* Tutorial
** Paper Reading
*** TODO [[/Users/zhangli/Documents/Library.papers3/Files/D6/D6FD20F2-226C-49D9-B4DB-FF1AF8C9C987.pdf][Caffe: Convolutional Architecture for Fast Feature Embedding]]
    SCHEDULED: <2016-08-24 Wed>
*** TODO [[/Users/zhangli/Documents/Library.papers3/Files/73/7398D9FD-507C-42B8-A5C1-07CABA329B0D.pdf][Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift]] [[http://arxiv.org/abs/1502.03167][{arxiv}]] 
** Linux Note
*** sed
#+BEGIN_SRC sh
  # from line 90 to line 100
  sed -n '90, 100p' file.txt

  # line 100
  sed -n '100, 1p' file.txt

  # from line a to line b
  sed -n 'a, bp' file.txt

  # if a > b return line a
#+END_SRC
*** nvidia installation
#+BEGIN_SRC sh
sudo add-apt-repository ppa:graphics-drivers/ppa

sudo apt-get update && sudo apt-get install nvidia-355
#+END_SRC
*** apt-get list installed packages
#+BEGIN_SRC sh
apt list --installed
#+END_SRC
*** list all users
#+BEGIN_SRC sh
  sed 's/:.*//' /etc/passwd
#+END_SRC
*** list all usergroup =groups=
*** usermod
**** add =newuser= to group =staff=
#+BEGIN_SRC sh
sudo usermod -G staff newuser
#+END_SRC
**** modify =newuser= to =newuser1=
#+BEGIN_SRC sh
sudo usermod -l newsuer1 newuser
#+END_SRC
*** watch GPU
#+BEGIN_SRC sh
watch -n 0.5 nvidia-smi
#+END_SRC
*** install =nvidia= driver
#+BEGIN_SRC sh
sudo add-apt-repository ppa:graphics-drivers/ppa

sudo apt-get update && sudo apt-get install nvidia-355
#+END_SRC
*** inspect symbol in =.so=
    #+BEGIN_SRC sh
    objdump -tT libName.so | grep symbol
    #+END_SRC
*** [[http://blog.csdn.net/wooin/article/details/580679][ =ldconfig= ]]
** Tmux Note
*** =tmux= plugin installation
    Add plugin to the list of TPM plugins in =.tmux.conf=:
#+BEGIN_SRC sh
  set -g @plugin 'tmux-plugins/tmux-pain-control'
#+END_SRC
Hit =prefix + I= to fetch the plugin and source it.
*** move panel to a new window
    =prefix + : break-panel=
** Deep learning Note
*** TODO [[http://karpathy.github.io/neuralnets/][Karpathy cnn tutorial]]
   SCHEDULED: <2016-08-21 Sun>
*** DONE [[http://karpathy.github.io/2015/05/21/rnn-effectiveness/][RNN tutorial]]
   CLOSED: [2016-09-02 Fri 21:50] SCHEDULED: <2016-08-21 Sun>
*** TODO [[https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html][vanishing gradient]]
   SCHEDULED: <2016-08-22 Mon>
*** softmax loss for one example
    \[p_k = \frac{e^{f_k}}{\sum_j e^{f_j}}\]
    \[L_i = -log(p_{y_i})\]
    \[\frac{\partial L_i}{\partial f_k} = 1 - I(y_i = k)\]

     Suppose the probabilities we computed were =p = [0.2, 0.3, 0.5]=,
    and that the correct class was the middle one (with probability
    0.3). According to this derivation the gradient on the scores
    would be =df = [0.2, -0.7, 0.5]=.
    
    #+BEGIN_SRC python
      dscores = probs
      dscores[range(num_examples),y] -= 1
      dscores /= num_examples

      dW = np.dot(X.T, dscores)
      db = np.sum(dscores, axis=0, keepdims=True)
      dW += reg*W # don't forget the regularization gradient
    #+END_SRC
*** TODO [[http://sebastianruder.com/optimizing-gradient-descent/][gradient descent]] 
    SCHEDULED: <2016-08-23 Tue>
**** [[https://www.quora.com/What-is-the-vanishing-gradient-problem][Quora answer]]
- Problem

Gradient based methods learn a parameter's value by understanding how
a small change in the parameter's value will affect the network's
output. If a change in the parameter's value causes very small change
in the network's output - the network just can't learn the parameter
effectively, which is a problem. 

This is exactly what's happening in the vanishing gradient problem --
the gradients of the network's output with respect to the parameters
in the early layers become extremely small. That's a fancy way of
saying that even a large change in the value of parameters for the
early layers doesn't have a big effect on the output. Let's try to
understand when and why does this problem happen. 

- Cause

Vanishing gradient problem depends on the choice of the activation
function. Many common activation functions (e.g sigmoid or tanh)
'squash' their input into a very small output range in a very
non-linear fashion. For example, sigmoid maps the real number line
onto a "small" range of [0, 1]. As a result, there are large regions
of the input space which are mapped to an extremely small range. In
these regions of the input space, even a large change in the input
will produce a small change in the output - hence the gradient is
small. 

This becomes much worse when we stack multiple layers of such
non-linearities on top of each other. For instance, first layer will
map a large input region to a smaller output region, which will be
mapped to an even smaller region by the second layer, which will be
mapped to an even smaller region by the third layer and so on. As a
result, even a large change in the parameters of the first layer
doesn't change the output much. 

We can avoid this problem by using activation functions which don't
have this property of 'squashing' the input space into a small
region. A popular choice is Rectified Linear Unit which maps   
x to max(0,x) .

Hopefully, this helps you understand the problem of vanishing
gradients. I'd also recommend reading along this iPython notebook
which does a small experiment to understand and visualize this
problem, as well as highlights the difference between the behavior of
sigmoid and rectified linear units. 
*** TODO [[http://colah.github.io/][deep learning blog (colah)]]
    SCHEDULED: <2016-08-25 Thu>
*** [[https://www.quora.com/How-do-I-learn-deep-learning-in-2-months][deep learning resource from quora]]
*** backward
**** [[https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html][backward gradient for batch normalization]]

       \[ y_i = BN_{r, \beta}(x_i) \] 
       \[ \mu_{}_{}_{}_\Beta \leftarrow \frac{1}{m}\sum_{i-1}^m x_i \]  // mini-batch mean
       \[ \sigma_{\Beta}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}(x_i - \mu_{\Beta})^2 \]  // mini-batch variance
       \[ \hat{x}_{i}  \leftarrow \frac{(x_i - \mu{}_{\Beta})^2}{\sqrt{\sigma_{\Beta}^2 + \epsilon }} \]  // normalize
       \[ y_i \leftarrow \gamma\hat{x}_i + \beta \]  // scale and shift
*** confusion matrix
**** [[http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/][tutorial]]
*** why dropout layer 
**** Dropout is a form of regularisation.[[https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning][(quora)]]
***** How does it work?
      It essentially forces an artificial neural network to learn multiple
      independent representations of the same data by alternately randomly
      disabling neurons in the learning phase. 
***** What is the effect of this?
      The effect of this is that neurons are prevented from co-adapting too
      much which makes overfitting less likely. 
***** Why does this happen?
      The reason that this works is comparable to why using the mean outputs
      of many separately trained neural networks to reduces overfitting. 
*** what we should care about in deep learning
**** One time setup
***** activation functions
****** sigmod function
******* Saturated neurons “kill” the gradients
******* Sigmoid outputs are not zero-centered
******* Consider what happens when the input to a neuron is always positive? What can we say about the gradients on w? Always all positive or all negative :(
(this is also why you want zero-mean data!)
******* exp() is a bit compute expensive
****** tanh(x)
******* Squashes numbers to range [-1,1]
******* zero centered (nice)
******* still kills gradients when saturated :(
****** ReLU (Rectified Linear Unit)
******* Does not saturate (in +region)
******* Converges much faster than
******* sigmoid/tanh in practice (e.g. 6x)
******* Very computationally efficient
******* Not zero-centered output
******* hint: what is the gradient when x < 0?
******* people like to initialize ReLU neurons with slightly positive biases (e.g. 0.01)
****** Leaky ReLU  f(x) = max(0.01x, x)
******* Does not saturate 
******* Computationally efficient Converges much faster than sigmoid/tanh in practice! (e.g. 6x)
******* will not “die”.
****** Parametric Rectifier (PReLU) f(x) = max(\alpha*x, x)
******* backprop into \alpha (parameter)
****** Exponential Linear Units (ELU)
******* \[ \begin{cases} x \ \ if\ x > 0 \\ \alpha (exp(x) - 1) \ \ if\ x \leq 0 \end{cases} \]
******* All benefits of ReLU
******* Does not die
******* Closer to zero mean outputs
******* Computation requires exp()
****** Maxout “Neuron” 
******* \[ max(w_1^{T}x + b_1, w_2^{T}x + b_2) \]
******* Does not have the basic form of dot product -> nonlinearity
******* Generalizes ReLU and Leaky ReLU
******* Linear Regime! Does not saturate! Does not die!
******* Problem: doubles the number of parameters/neuron :(
***** preprocessing
***** weight initialization
****** Small random numbers (gaussian with zero mean and 1e-2 standard deviation)
       #+BEGIN_SRC python
         W = 0.01 * np.random.randn(D, H)
       #+END_SRC
******* Works ~okay for small networks, but can lead to non-homogeneous distributions of activations across the layers of a network.
******* 10-layer net with 500 neurons on each layer, using tanh non-linearities
******** All activations become zero!
******** Almost all neurons completely saturated, either -1 and 1. Gradients will be all zero.
****** Xavier initialization [Glorot et al., 2010]
       #+BEGIN_SRC python
         W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)
       #+END_SRC
******* Reasonable initialization
******* but when using the ReLU nonlinearity it breaks.
****** He et al., 2015
       #+BEGIN_SRC python
         W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in / 2)
       #+END_SRC
****** Proper initialization is an active area of research
******* Understanding the difficulty of training deep feedforward neural networks by Glorot and Bengio, 2010 
******* Exact solutions to the nonlinear dynamics of learning in deep linear neural networks by Saxe et al, 2013
******* Random walk initialization for training very deep feedforward networks by Sussillo and Abbott, 2014
******* Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification by He et al., 2015
******* Data-dependent Initializations of Convolutional Neural Networks by Krähenbühl et al., 2015
******* All you need is a good init, Mishkin and Matas, 2015
***** layer tricks
****** Batch Normalization
******* consider a batch of activations at some layer. To make each dimension unit gaussian, apply:
        \[ \hat{x}_{(k)}} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}} \]
******* his is a vanilla differentiable function...
******* compute the empirical mean and variance independently for each dimension.
******* Normalize
******* Usually inserted after Fully Connected / (or Convolutional, as we’ll see soon) layers, and before nonlinearity.
******* Problem: do we necessarily want a unit gaussian input to a tanh layer?
******* advantages
******** improves gradient flow through the network
******** Allows higher learning rates
******** Reduces the strong dependence on initialization
******** Acts as a form of regularization in a funny way, and slightly reduces the need for dropout, maybe
******* algorithm
        \[ y_i = BN_{r, \beta}(x_i) \] 
        \[ \mu_{}_{}_{}_\Beta \leftarrow \frac{1}{m}\sum_{i-1}^m x_i \]  // mini-batch mean
        \[ \sigma_{\Beta}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}(x_i - \mu_{\Beta})^2 \]  // mini-batch variance
        \[ \hat{x}_{i}  \leftarrow \frac{(x_i - \mu{}_{\Beta})^2}{\sqrt{\sigma_{\Beta}^2 + \epsilon }} \]  // normalize
        \[ y_i \leftarrow \gamma\hat{x}_i + \beta \]  // scale and shift
******* Note: at test time BatchNorm layer functions differently:
        - The mean/std are not computed based on the batch. Instead, a
          single fixed empirical mean of activations during training
          is used.
        - (e.g. can be estimated during training with running averages)
***** regularization
****** Dropout
       - randomly set some neurons to zero in the forward pass
         #+BEGIN_SRC python
           p = 0.5  # probability of keeping a unit active. high = less dropout

           def train_step(X):
               """ X contains the data """

               # forward pass for example 3-layer neural network
               H1 = np.maximum(0, np.dot(W1, X) + b1)
               U1 = np.random.rand(*H1.shape) < p  # first dropout mask
               H1 *= U1  # drop!
               H2 = np.maximum(0, np.dot(W2, H1) + b2)
               U2 = np.random.rand(*H2.shape) < p  # second dropout mask
               H2 *= U2  # drop!
               out = np.dot(W3, H2) + b3

               # backward pass: compute gradients... (not shown)
               # perform paramter update... (not shown)
         #+END_SRC
******* How could this possibly be a good idea?
        - Forces the network to have a redundant representation
        - Dropout is training a large ensemble of models (that share parameters).
        - Each binary mask is one model, gets trained on only ~one
          datapoint.
******* At test time...
        - Ideally: want to integrate out all the noise
        - Monte Carlo approximation: do many forward passes with
          different dropout masks, average all predictioins
        - Can in fact do this with a single forward pass! Leave all
          input neurons turned on (no dropout).
          + during test : a = W0*x + W1*y
          + during train:
            E[a] = 1/4*(W0*0 + W1*0 + W0*0 + W1*y + W0*x
            + W1*0 + W0*x + W1*y) = 1/4*(2W0*x + 2W1*y) = 1/2(W0*x +
              W1*y)
          + with p = 0.5, using all inputs in the forward pass would
            inflate the activations by 2x from what the network was
            "used to" during training! => Have to compensate by
            scaling the activations back down by 1/2.
          + At test time all neurons are active always => output at
            test time = expected output at training time
            #+BEGIN_SRC python
              def predict(X):
                  # ensembled forward pass
                  H1 = np.maximum(0, np.dot(W1, X) + b1) * p  # NOTE: scale the activations
                  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p  # NOTE: scale the activations
                  out = np.dot(W3, H2) + b3
            #+END_SRC
            
            #+BEGIN_SRC python
              p = 0.5  # probability of keeping a unit active. higher = less dropout

              def train_step(X):
                  # forward pass for example 3-layer neural network
                  H1 = np.maximum(0, np.dot(W1, X) + b1)
                  U1 = (np.random.rand(*H1.shape) < p) / p  # first dropout mask. Notice /p!
                  H1 *= U1  # drop!
                  H2 = np.maximum(0, np.dot(W22, H1) + b2)
                  U2 = (np.random.rand(*H2.shape) < p) / p  # second dropout mask. Notice /p!
                  H2 *= U2  # drop!
                  out = np.dot(W3, H2) + b3



              def predict(X):
                  # ensembled forward pass
                  H1 = np.maximum(0, np.dot(W1, X) + b1)  # no scaling necessary
                  H2 = np.maximum(0, np.dot(W2, H1) + b2)
                  out = np.dot(W3, H2) + b3
            #+END_SRC

***** gradient checking
**** Training dynamics
***** babysitting the learning process
****** Double check that the loss is reasonable
       #+BEGIN_SRC python
         import numpy as np


         def init_two_layer_model(input_size, hidden_size, output_size):
             # initialize a model
             model = {}
             model['W1'] = 0.0001 * np.random.randn(input_size, hidden_size)
             model['b1'] = np.zeros(hidden_size)
             model['W2'] = 0.0001 * np.random.randn(hidden_size, output_size)
             model['b2'] = np.zeros(output_size)
             return model


         def two_layer_net(X_train, model, y_train, r):
             ''' returns the loss and the gradient for all parameters '''
             pass


         model = init_two_layer_model(32*32*3, 50, 10)  # input_size, hidden_size, number of classes
         loss, grad = two_layer_net(X_train, model, y_train, 0.0)  # diable regularization

         # loss 2.3026121617 loss ~2.3 'correct' for 10 classes


         model = init_two_layer_model(32*32*3, 50, 10)  # input_size, hidden_size, number of classes
         loss, grad = two_layer_net(X_train, model, y_train, 1e3)  # crank up regularization

         # 3.06859716482 loss went up, good (sanity check)

       #+END_SRC
****** Make sure that you can overfit very small portion of the training data
       #+BEGIN_SRC python
         model = init_two_lay_model(32*32*3, 50, 10)
         trainer = ClassifierTrainer()
         x_tiny = x_train[:20]  # take 20 examples
         y_tiny = y_train[:20]
         best_model, stats = trainer.train(x_tiny, y_tiny,
                                           model, two_layer_net,
                                           update='sgd', learning_rate_decay=1,
                                           sample_batches=False,
                                           learning_rate=1e-3, verbose=True)
       #+END_SRC
       - take the first 20 examples from CIFAR-10
       - turn off regularization (reg = 0.0)
       - use simple vanilla 'sgd'
       - very small loss, train accuracy 1.00, nice!
****** I like to start with small regularization and find learning rate that makes the loss go down.
       #+BEGIN_SRC python
         model = init_two_lay_model(32*32*3, 50, 10)
         trainer = ClassifierTrainer()
         x_tiny = x_train[:20]  # take 20 examples
         y_tiny = y_train[:20]
         best_model, stats = trainer.train(x_tiny, y_tiny,
                                           model, two_layer_net,
                                           update='sgd', learning_rate_decay=1,
                                           sample_batches=False,
                                           learning_rate=1e-6, verbose=True)

         # Loss barely changing: Learning rate is probably too low
         # Notice train/val accuracy goes up

       #+END_SRC
****** loss not going down: learning rate too low
****** loss exploding: learning rate too high
****** cost: NaN almost always means high learning rate...
****** Rough range for learning rate we should be cross-validating is somewhere [1e-3 ... 1e-5]
***** parameter updates
****** Mini-batch SGD
******* loop:
        1. Sample a batch of data
        2. Forward prop it through the graph, get loss
        3. Backprop to calculate the gradients
        4. Update the parameters using the gradient
           #+BEGIN_SRC python
             while True:
                 data_batch = dataset.sample_data_batch()
                 loss = network.forward(data_batch)
                 dx = network.backward()
                 x += - learning_rate * dx
           #+END_SRC
****** Momentum update
       #+BEGIN_SRC python
         # Gradient descent update
         x += - learning_rate * dx

         # Momentum update
         v = mu * v - learning_rate * dx  # integrate velocity
         x += v  # integrate position

       #+END_SRC
       - Physical intepretation as ball rolling down the loss
         function + friction (\mu coefficient)
       - \mu = usually ~ 0.5, 0.9, or 0.99 (sometimes anneled over time,
         e.g. from 0.5 -> 0.99)
       - Allows a velocity to build up along shallow directions
       - Velocity becomes damped in steep direction due to quickly
         changing sign
****** Nesterov Momentum update
       v_t = \mu v_{t-1} - \epsilon \nabla f(\theta_{t-1} + \mu v_{t-1})

       \theta_t = \theta_{t-1} + v_t 
       
       \theta_{t-1} + \mu v_{t-1} Slightly inconvenient usually we have : \theta_{t-1}
       \nabla f(\theta_{t-1})  
       - variable transform and rearranging saves the day: \phi_{t-1} = \theta_{t-1} + \mu v_{t-1}
       - replace all \theta  with \phi , rearrange and obtain:

         v_t = \mu v_{t-1} - \epsilon \nabla f(\phi_{t-1})

       \phi_t = \phi_{t-1} - \mu v_{t-1} + (1 + \mu)v_t 
       #+BEGIN_SRC python
         # Nesterov momentum update rewrite
         v_prev = v
         v = mu * v - learning_rate * dx
         x += -mu * v_prev + (1 + mu) * v
       #+END_SRC
****** AdaGrad update
       - Added element-wise scaling of the gradient based on the
         historical sum of squares in each dimension
         #+BEGIN_SRC python
           # Adagrad update

           cache += dx**2
           x += - learning_rate * dx / (np.sqrt(cache) + 1e-7)
         #+END_SRC
****** RMSProp update
       #+BEGIN_SRC python
         # Adagrad update
         cache += decay_rate * cache + (1 - decay_rate) * dx**2
         x += - learning_rate * dx / (np.sqrt(cache) + 1e-7)
       #+END_SRC
****** Adam update
       #+BEGIN_SRC python
         # Adam
         m = beta1*m + (1 - beta1)*dx  # update first moment momentum
         v = beta2*v + (1 - beta2)*(dx**2)  # update second moment RMSProp-like

         x += - learning_rate * m / (np.sqrt(v) + 1e-7)
       #+END_SRC

       #+BEGIN_SRC python
         # Adam
         m, v =  # ... initialize caches to zeros
         for t in xrange(1, big_number):
             dx =  # ... evaluate gradient
             m = beta1 * m + (1 - beta1) * dx  # update first moment
             v = beta2 * v + (1 - beta2) * (dx**2)  # update second moment
             mb = m / (1 - beta1**t)  # correct bias
             vb = v / (1 - beta2**t)  # correct bias
             x += - learning_rate * mb / (np.sqrt(vb) + 1e-7)
       #+END_SRC
       - The biaas correction compensates for the fact that m, v are
         initialize at zero and need some time to "warm up".
       - bias correction only relevant in first few iteration when t
         is small
****** learning rate as a hyperparameter => learning rate decay over time! 
       - step decay: e.g. decay learning rate by half every few epochs
       - exponential decay: \[ \alpha = \alpha_0 e^{-kt }\]
       - 1/t decay: \[ \alpha = \alpha_0 / (1 + kt) \]
****** Adam is a good default choice in most cases
***** hyperparameter optimization
****** network architecture
****** learning rate, its decay schedule, update type
****** regularization (L2/Droout strength)
****** monitor and visualize the loss curve
****** monitor and visualize the accuracy
       - big gap = overfitting => increase regularization strength?
       - no gap => increase model capacity?
****** Track the ratio of weight updates / weight magnitudes:
       #+BEGIN_SRC python
         # assume parameter vector W and its gradient vector dW
         param_scale = np.linalg.norm(W.ravel())
         update = -learning_rate*dW  # simple SGD update
         update_scale = np.linalg.norm(update.ravel())
         W += update  # the actual update
         print update_scale / param_scale  # want -1e-3
       #+END_SRC
       - atio between the values and updates: ~ 0.0002 / 0.02 = 0.01 (about okay)
       - want this to be somewhere around 0.001 or so
****** Cross-validation strategy
******* I like to do coarse -> fine cross-validation in stages
        - First stage: only a few epochs to get rough idea of what params work
        - Second stage: longer running time, finer search ... (repeat
          as necessary)
          #+BEGIN_SRC python
            max_count = 100

            # coarse search
            for count in xrange(max_count):
                # note it's best to optimize in log space!
                reg = 10**uniform(-5, 5)
                lr = 10**uniform(-3, -6)

            # fine search
            for count in xrange(max_count):
                reg = 10**uniform(-4, 0)
                lr = 10**uniform(-3, -4)
          #+END_SRC

**** Evaluation
***** model ensembles
      1. Train multiple independent models
      2. At test time average their results (Enjoy 2% extra performace)
      3. Fun Tips/Tricks:
         - can also get a small boost from averaging multiple model
           checkpoints of a single model.
         - keep track of (and use at test time) a running average
           parameter vector:
           #+BEGIN_SRC python
             while True:
                 data_batch = dataset.sample_data_batch()
                 loss = network.forward(data_batch)
                 dx = network.backward()
                 x += - learning_rate * dx
                 x_test = 0.9995*x_test + 0.005*x  # use for test set
           #+END_SRC
** Caffe Note
*** [[https://github.com/BVLC/caffe/tree/85bb397acfd383a676c125c75d877642d6b39ff6/examples/feature_extraction][extract feature]]
**** using caffe to extract features
    #+BEGIN_SRC sh
      find `pwd`/examples/images -type f -exec echo {} \; > examples/_temp/temp.txt
      sed "s/$/ 0/" examples/_temp/temp.txt > examples/_temp/file_list.txt
      cd $CAFFE
      ./build/tools/extract_features models/bvlc_reference_caffenet/bvlc_reference examples/_temp/imagenet_val.prototxt example/_temp/feature fc7 10 lmdb GPU 0
    #+END_SRC
**** general command for extract feature using caffe
#+BEGIN_SRC sh
  extract_features pretrained_net_param  feature_extraction_proto_file \
  extract_feature_blob_name1[,name2,...]  save_feature_dataset_name1[,name2,...] \
  num_mini_batches  db_type  [CPU/GPU] [DEVICE_ID=0]
#+END_SRC
- 参数1是模型参数（.caffemodel）文件的路径。

- 参数2是描述网络结构的prototxt文件。程序会从参数1的caffemodel文件里找
  对应名称的layer读取参数。 

- 参数3是需要提取的blob名称，对应网络结构prototxt里的名称。blob名称可
  以有多个，用逗号分隔。每个blob提取出的特征会分开保存。 

- 参数4是保存提取出来的特征的数据库路径，可以有多个，和参数3中一一对应，
  以逗号分隔。如果用LMDB的话，路径必须是不存在的（已经存在的话要改名或
  者删除）。 


- 参数5是提取特征所需要执行的batch数量。这个数值和prototxt里DataLayer
  中的Caffe的DataLayer(或者ImageDataLayer)中的batch_size参数相乘，就是
  会被输入网络的总样本数。设置参数时需要确保batch_size *
  num_mini_batches等于需要提取特征的样本总数，否则提取的特征就会不够数
  或者是多了。 


- 参数6是保存特征使用的数据库类型，支持lmdb和leveldb两种(小写)。推荐使用
lmdb，因为lmdb的访问速度更快，还支持多进程同时读取。 

- 参数7决定使用GPU还是CPU，直接写对应的三个大写字母就行。省略不写的话默
认是CPU。 

- 参数8决定使用哪个GPU，在多GPU的机器上跑的时候需要指定。省略不写的话默
认使用0号GPU。 

注意事项
- 提取特征时，网络运行在Test模式下
    * Dropout层在Test模式下不起作用，不必担心dropout影响结果
    * Train和Test的参数写在同一个Prototxt里的时候，改参数的时候注意不
      要改错地方(比如有两个DataLayer的情况下) 
- 减去均值图像
    * 提取特征时，输入的图像要减去均值
    * 应该减去训练数据集的均值
- 提取哪一层
    * 不要提取Softmax网络的最后一层(如AlexNet的fc8)，因为最后一层已经
      是分类任务的输出，作为特征的可推广性不够好
**** read from lmdb
    #+BEGIN_SRC pytho
      import numpy as np
      import caffe
      import lmdb
      from caffe.proto import caffe_pb2

      fea_lmdb = lmdb.open('featureA')
      lmdb_txn = fea_lmdb.begin()
      lmdb_cursor = lmdb_txn.cursor()
      features = []

      for key, value in lmdb_cursor:
          datum = caffe_pb2.Datum()
          datum.ParseFromString(value)
          data = caffe.io.datum_to_array(datum)
          features.append(data)

    #+END_SRC
**** image recognition using =cos= similarity measure
#+BEGIN_SRC python

  import numpy as np
  import caffe
  import lmdb
  from caffe.proto import caffe_pb2
  from scipy import spatial


  # 3 steps to read form lmdb
  fea_lmdb = lmdb.ope




n('/root/caffe/examples/_temp/featureA')
  lmdb_txn = fea_lmdb.begin()
  lmdb_cursor = lmdb_txn.cursor()
  features = []

  for key, value in lmdb_cursor:
      datum = caffe_pb2.Datum()
      # Parse from serialized data
      datum.ParseFromString(value)
      data = caffe.io.datum_to_array(datum)
      features.append(data)

  out = []
  for f in features:
      out.append(f.flatten())

  n = len(out)
  similarity = np.zeros((n, n), dtype=np.double)

  for i in xrange(n):
      for j in xrange(n):
        # cosin distance
          similarity[i, j] = 1 - spatial.distance.cosine(out[i], out[j])

#+END_SRC
**** =cos= similarity result
- accuracy (true ture) : 53 / 55
#+BEGIN_SRC python
a = similarity[0:10, 0:10]
  array([[ 1.        ,  0.63231419,  0.84345085,  0.73587363,  0.58211244,
           0.67306891,  0.46881317,  0.56938226,  0.65432654,  0.55240935],
         [ 0.63231419,  1.        ,  0.68508232,  0.56741804,  0.74116358,
           0.81706845,  0.71951714,  0.75391089,  0.78529276,  0.74174079],
         [ 0.84345085,  0.68508232,  1.        ,  0.78416825,  0.61635946,
           0.72695667,  0.54473343,  0.60050371,  0.70046374,  0.58715887],
         [ 0.73587363,  0.56741804,  0.78416825,  1.        ,  0.50801387,
           0.60814318,  0.5046651 ,  0.52948304,  0.68054069,  0.49502061],
         [ 0.58211244,  0.74116358,  0.61635946,  0.50801387,  1.        ,
           0.88589477,  0.56183335,  0.72687896,  0.60917844,  0.87135289],
         [ 0.67306891,  0.81706845,  0.72695667,  0.60814318,  0.88589477,
           1.        ,  0.63597132,  0.76000156,  0.7042399 ,  0.87401555],
         [ 0.46881317,  0.71951714,  0.54473343,  0.5046651 ,  0.56183335,
           0.63597132,  1.        ,  0.58212342,  0.64319046,  0.6254508 ],
         [ 0.56938226,  0.75391089,  0.60050371,  0.52948304,  0.72687896,
           0.76000156,  0.58212342,  1.        ,  0.74652927,  0.72233884],
         [ 0.65432654,  0.78529276,  0.70046374,  0.68054069,  0.60917844,
           0.7042399 ,  0.64319046,  0.74652927,  1.        ,  0.61672591],
         [ 0.55240935,  0.74174079,  0.58715887,  0.49502061,  0.87135289,
           0.87401555,  0.6254508 ,  0.72233884,  0.61672591,  1.        ]])

np.sum(a > 0.5)
96
#+END_SRC
- false true : 2 / 100
#+BEGIN_SRC python
In [1]: ab = similarity[0:10, 10:]

In [2]: ab
Out[2]:
array([[ 0.2842583 ,  0.37596221,  0.27628312,  0.12041221,  0.29636999,
         0.13618284,  0.1381707 ,  0.17832465,  0.21937008,  0.40752771],
       [ 0.32961919,  0.49064045,  0.29595205,  0.093565  ,  0.39657901,
         0.17370467,  0.15514055,  0.2672414 ,  0.31652746,  0.46922921],
       [ 0.31926577,  0.45413662,  0.26234978,  0.1560283 ,  0.30816957,
         0.15273065,  0.16850629,  0.22604249,  0.25764858,  0.44164225],
       [ 0.26623039,  0.3611369 ,  0.20121232,  0.11351721,  0.21726182,
         0.11916629,  0.1431136 ,  0.20710409,  0.22387793,  0.31652456],
       [ 0.30927462,  0.35910132,  0.2650208 ,  0.08663475,  0.37263798,
         0.10722143,  0.09815253,  0.17950735,  0.20988739,  0.50689106],
       [ 0.32089366,  0.40492257,  0.28595893,  0.09466663,  0.37709065,
         0.10737807,  0.10595637,  0.19340299,  0.23139416,  0.51704389],
       [ 0.29795872,  0.3890121 ,  0.26349005,  0.08589599,  0.36945176,
         0.16923292,  0.11844475,  0.24970864,  0.31689723,  0.36337912],
       [ 0.28911623,  0.33516171,  0.30897566,  0.12046317,  0.36436887,
         0.10022814,  0.14957088,  0.29092572,  0.3343103 ,  0.47673998],
       [ 0.31926479,  0.43550698,  0.31588098,  0.09185497,  0.33737191,
         0.15741605,  0.16819127,  0.34134218,  0.38785466,  0.41883917],
       [ 0.29190126,  0.3130953 ,  0.25801771,  0.07097081,  0.34608239,
         0.09577894,  0.0842366 ,  0.14185045,  0.19112799,  0.47368384]])

In [3]: np.sum(ab > 0.5)
Out[3]: 2

#+END_SRC
*** Autoencoders
**** TODO [[http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/][UFLDL]]
     SCHEDULED: <2016-08-25 Thu>
*** TODO [[https://github.com/soumith/convnet-benchmarks][convnet benchmarks]]
    SCHEDULED: <2016-09-09 Fri>
** Emacs Note
*** =elpy=
**** use-package =elpy=
#+BEGIN_SRC lisp
  ;; bind return key with <RET>, must be capitalized
  (:bind (M-<RET> . elpy-shell-send-current-statement))
#+END_SRC
**** elpy send function definition =C-M-x=
*** =ac-c-headers= locate =c= headers
#+BEGIN_SRC sh
gcc -xc++ -E -v -
#+END_SRC
#+BEGIN_SRC elisp
  (add-to-list achead:include-directories "/usr/include")
#+END_SRC
*** [[http://tuhdo.github.io/][tutorial]]
*** etags
    #+BEGIN_SRC sh
      find . -name "*.cpp" -print -o -name "*.h" -print | etags -  
    #+END_SRC
*** =babel-language=
    - directory =~/.emacs.d/elpa/org-20160620=
    - add =ob-lang.el=
*** =query-replace-regexp= [[https://www.gnu.org/software/emacs/manual/html_node/emacs/Regexp-Replace.html][note]]
*** =impatient-mode=
    #+BEGIN_SRC elisp
      ;; impatient-mode
      ;; useage: httpd start impatient-mode
      ;; localhost:8080/imp
      (use-package impatient-mode
        :ensure t
        :config
        (require 'impatient-mode))
    #+END_SRC
** =Org-mode= Note
*** move item up/down =M-up= =org-metaup=
*** change all level to next level =M-shift-left=
*** =C-x n s= (org-narrow-to-subtree)
*** =C-x n w= (widen)
*** literate programming in =org mode=
**** TODO [[http://www.howardism.org/Technical/Emacs/literate-devops.html][literate programming howard abrams]]
     SCHEDULED: <2016-09-13 Tue>
**** TODO [[http://www.howardism.org/Technical/LP/introduction.html][introduction Howard Abrams]]
     SCHEDULED: <2016-09-13 Tue>
*** show utf-8 character =C-c C-x \=
*** latex
**** [[http://orgmode.org/worg/org-tutorials/org-latex-preview.html][latex configuration]]
**** =C-c C-x C-l= latex preview in org
**** latex =tangle=
#+BEGIN_SRC latex :tangle example.tex
  \documentclass{article}

  \begin{document}

  \[
  e^{i\pi} = -1
  \]

  \[
  \int_0^\infty e^{-x^2} dx = \frac{\sqrt{\pi}}{2}
  \]

  \end{document}

#+END_SRC
*** org-narrow-forward
    #+BEGIN_SRC elisp
      ;; org-narrow-forward
      (defun zl/org-narrow-forward ()
        "Move to the next subtree at same level, and narrow to it."
        (interactive)
        (widen)
        (org-forward-heading-same-level 1)
        (org-narrow-to-subtree))

      (defun zl/set-org-keys ()
          (local-set-key "\C-xnm" 'zl/org-narrow-forward))

      (add-hook 'org-mode-hook 'zl/set-org-keys)
    #+END_SRC
** Python Note
*** sample from an nd-array
#+BEGIN_SRC python
# sample from an nd-arrary
numpy.random.choice(range(vocab_size), p=p.ravel())
#+END_SRC
*** install package anaconda ubuntu
#+BEGIN_SRC sh
cd ~/anaconda2/bin
su
./pip install package
#+END_SRC
*** =sum= to keepdim
#+BEGIN_SRC python
np.sum(array, axis=0, keepdims=True)
#+END_SRC
*** =numpy.clip=
Clip (limit) the values in an array.

Given an interval, values outside the interval are clipped to the
interval edges. For example, if an interval of [0, 1] is specified,
values smaller than 0 become 0, and values larger than 1 become 1.

#+BEGIN_SRC python
>>> a = np.arange(10)
>>> np.clip(a, 1, 8)
array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8])
>>> a
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
>>> np.clip(a, 3, 6, out=a)
array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6])
>>> a = np.arange(10)
>>> a
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
>>> np.clip(a, [3,4,1,1,1,4,4,4,4,4], 8)
array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8])
#+END_SRC
*** [[https://pyformat.info/][python string format]]
** Ipython Note
*** ipython config
    #+BEGIN_SRC sh
      ipython profile create

      emacs -nw /Users/zhangli/.ipython/profile_default/ipython_config.py
    #+END_SRC
=c.TerminalInteractiveShell.confirm_exit = False=
*** ipython share kernel
    #+BEGIN_SRC ipython
    %connect_info
    #+END_SRC
    #+BEGIN_SRC sh
    ipython console --existing /Users/zhangli/Library/Jupyter/runtime/kernel-0f76f3a7-104c-49dc-8942-162b50f5799b.json
    #+END_SRC
** Docker Note
*** docker installation
#+BEGIN_SRC sh
  wget -qO- https://get.docker.com/ | sh
  sudo usermod -aG docker sxwl1080
#+END_SRC
*** docker sourcelist
ubuntu: /etc/default/docker 
#+BEGIN_SRC example
DOCKER_OPTS="--dns 8.8.8.8 --dns 8.8.4.4 --insecure-registry dl.dockerpool.com:5000
#+END_SRC
*** docker push
    unauthorized: authentication required
    sudo docker login --username=xiaoxinyi
*** docker machine installation
#+BEGIN_SRC sh
  curl -L https://github.com/docker/machine/releases/download/v0.7.0/docker-machine-`uname -s`-`uname -m` > /usr/local/bin/docker-machine  
#+END_SRC
*** docker swarm
    - [[http://blog.arungupta.me/clustering-docker-swarm-techtip85/][tutorial]]
** Proxy
*** proxychains
#+BEGIN_SRC sh
sudo apt-get install -y proxychains
sudo cat "socks5  127.0.0.1 9999" >> /etc/proxychains.conf
ssh -p 1022  -fN -D 127.0.0.1:9999 root@192.168.199.1
proxychains curl www.google.co.jp
#+END_SRC
** Cuda Note
*** Configuring the kernel launch
kernel<<<grid of block, block of threads>>>(...)
square<<<dim3(bx,by,bz), dime(tx,ty,tz), sharem>>>(...)

grid of blocks : bx * by * bz
block of threads : tx * ty * tz
shared memory per block in bytes
*** Convert color to black and white
I = (R + G + B) / 3
I = .299f * R + .587f * G + .114f * B
*** [[http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#cuda-programming-model][ =nvcc= introduction]]
*** cs344 Note
- GPU is responsible for allocating blocks to SM(streaming multiprocessors)
- A block cannot run on more than one SM
- An SM may run more than one block
- All the SMs are running in parallel
- Threads in different block shouldn't cooperate
- Cuda make few guarantees about when and where thread blocks will run
- consequences
  + no assumptions blocks -> SM
  + no communication between blocks
- CUDA guarantees that:
  + all threads in a block run on the same SM at the same time
  + all blocks in a kernel finish before any blocks from next run
- threadIdx : thread within block threadIdx.x threadIdx.y
  + blockDim : size of block
  + blockIdx : block within grid
  + gridDim : size of grid
*** GPU memory model
[[./images/gpu-memory-model.png]]
  * All threads from a block can access the same variable in that
    block shared memory
  * Threads from two different blocks can access the same variable in
    global memory
  * Threads from different blocks have their own copy of local
    variables in local memory
  * Threads from the same block have their own copy of local variables
    in local memory

*** barrier
point in program where threads stop and wait. when all threads have
reached the barrier, they can proceed.
[[./images/synchronized.png]]
*** High-level strategies
1. Maximize arithmetic intensity
\[\frac{Math}{Memory}\]
  - maximize compute ops per thread
  - minimize time spent on memory per thread
     + move frequently-accessed data to fast memory
       local > shared >> global >> cpu memory
coalesce memeory
[[./images/coalesce.png]]
2. avoid thread divergence

*** =cudaMalloc=
    #+BEGIN_SRC c++
      float *device_data=NULL;  
      size_t size = 1024*sizeof(float);  
      cudaMalloc((void**)&device_data, size);  
    #+END_SRC
而device_data这个指针是存储在主存上的。之所以取device_data的地址，是为
了将cudaMalloc在显存上获得的数组首地址赋值给device_data。在函数中为形
参赋值是不会在实参中繁盛变化的，但是指针传递的是地址 

*** TODO [[/Users/zhangli/Documents/Library.papers3/Files/1E/1ED49076-5D40-4E5F-B232-918B17EA1596.pdf][What Every Programmer Should Know About Memory]]
    SCHEDULED: <2016-08-27 Sat>







*** levels of optimization
**** Picking good algorithms 3 - 10x
     - use mergesort \[O(nlgn)\] vs insertion sort \[O(n^2)\]
**** Basic principles for efficiency 3 - 10x
     - write cache-aware code. e.g. traverse rows vs cols
**** Arch-specific detailed optimizations 30% - 80%
     - block for the L1 cache
     - vector register SSE, AVX
**** \[\mu\]-optimization at instruction level
     - float recipe =sqrt = (float)0x5f3659da - (a >> 1)=
*** profiler
    - gprof
    - vtune
    - verysleepy
*** Amdahl's law
    - total speedup from parallelization is limited by protion of time
      spent doing some thing to be parralledized 
    \[ max speedup -> \frac{1}{1 - p}  \] p is % parallelizable time
*** most GPU codes are memory limited, always start by measuring bandwith
** Http
*** [[http://www.imooc.com/article/3582][http tutorial imooc]]
*** HTTP: Get & Post
Http协议定义了很多与服务器交互的方法，最基本的有4种，分别是
GET,POST,PUT,DELETE. 一个URL地址用于描述一个网络上的资源，而HTTP中的
GET, POST, PUT, DELETE就对应着对这个资源的查，改，增，删4个操作。 我们
最常见的就是GET和POST了。GET一般用于获取/查询资源信息，而POST一般用于
更新资源信息. 
- GET提交的数据会放在URL之后，以?分割URL和传输数据，参数之间以&相连，
  如EditPosts.aspx?name=test1&id=123456. POST方法是把提交的数据放在
  HTTP包的Body中.
- GET提交的数据大小有限制（因为浏览器对URL的长度有限制），而POST方法提
  交的数据没有限制.
- GET方式需要使用Request.QueryString来取得变量的值，而POST方式通过
  Request.Form来获取变量的值，也就是说Get是通过地址栏来传值，而Post是
  通过提交表单来传值。
- GET方式提交数据，会带来安全问题，比如一个登录页面，通过GET方式提交数
  据时，用户名和密码将出现在URL上，如果页面可以被缓存或者其他人可以访
  问这台机器，就可以从历史记录获得该用户的账号和密码. 

** OSX
*** lsof
#+BEGIN_SRC sh
  lsof -i TCP:port -n 
  lsof -i UDP:port -n
＃ listen ports
lsof -iTCP -sTCP:LISTEN -n -P
lsof -i -n -P | grep -i listen
#+END_SRC
*** brew
    #+BEGIN_SRC sh :results output
      brew info imagemagick
    #+END_SRC

** Torch
*** TODO [[https://github.com/torch/nn/blob/master/doc/training.md#stochasticgradientmodule-criterion][gradient in torch]]
    SCHEDULED: <2016-08-31 Wed>
*** JIT complier
    In the beginning, a compiler was responsible for turning a
    high-level language (defined as higher level than assembler) into
    object code (machine instructions), which would then be linked (by
    a linker) into an executable. 

    At one point in the evolution of languages, compilers would compile a
    high-level language into pseudo-code, which would then be interpreted
    (by an interpreter) to run your program. This eliminated the object
    code and executables, and allowed these languages to be portable to
    multiple operating systems and hardware platforms. Pascal (which
    compiled to P-Code) was one of the first; Java and C# are more recent
    examples. Eventually the term P-Code was replaced with bytecode, since
    most of the pseudo-operations are a byte long. 

    A Just-In-Time (JIT) compiler is a feature of the run-time
    interpreter, that instead of interpreting bytecode every time a method
    is invoked, will compile the bytecode into the machine code
    instructions of the running machine, and then invoke this object code
    instead. Ideally the efficiency of running object code will overcome
    the inefficiency of recompiling the program every time it runs. 
*** install =fblualib=
**** git clone [[https://github.com/facebook/fblualib][repository]]
**** =./install_all.sh=
**** =fb.debugger=
     #+BEGIN_SRC lua
     local debugger = require('fb.debugger')
     debugger.enter()
     #+END_SRC
**** TODO =fblualib= [[https://github.com/facebook/fblualib][git repository]]
     SCHEDULED: <2016-09-11 Sun>
*** TODO [[https://github.com/facebook/fb.resnet.torch/tree/master/pretrained][fb.resnet.torch]]
    SCHEDULED: <2016-09-11 Sun>
*** [[https://github.com/torch/nngraph][ =nngraph= tutorial]]
*** TODO [[https://github.com/torch/demos][torch demos and tutorial]]
    SCHEDULED: <2016-09-12 Mon>
*** [[http://jucor.github.io/torch-doc-template/tensor.html][torch tensor reference]]
*** [[https://github.com/torch/torch7/wiki/Cheatsheet][torch cheatsheet]]
*** torch install packages
    #+BEGIN_SRC sh
      luarocks install torch-rocks install https://raw.github.com/andresy/mnist/master/rocks/mnist-scm-1.rockspec
    #+END_SRC
*** create a =nn= example
    #+BEGIN_SRC lua
      function createModel(nGPU)
         require 'cunn'

         local model = nn.Sequential()

         local function block(...)
            local arg = {...}
            local no = arg[2]
            model:add(nn.SpatialConvolution(...))
            model:add(nn.SpatialBatchNormalization(no,1e-3))
            model:add(nn.ReLU(true))
            model:add(nn.SpatialConvolution(no, no, 1, 1, 1, 1, 0, 0))
            model:add(nn.SpatialBatchNormalization(no,1e-3))
            model:add(nn.ReLU(true))
            model:add(nn.SpatialConvolution(no, no, 1, 1, 1, 1, 0, 0))
            model:add(nn.SpatialBatchNormalization(no,1e-3))
            model:add(nn.ReLU(true))
         end

         local function mp(...)
            model:add(nn.SpatialMaxPooling(...))
         end

         block(3, 96, 11, 11, 4, 4, 5, 5)
         mp(3, 3, 2, 2, 1, 1)
         block(96, 256, 5, 5, 1, 1, 2, 2)
         mp(3, 3, 2, 2, 1, 1)
         block(256, 384, 3, 3, 1, 1, 1, 1)
         mp(3, 3, 2, 2, 1, 1)
         block(384, 1024, 3, 3, 1, 1, 1, 1)

         model:add(nn.SpatialAveragePooling(7, 7, 1, 1))
         model:add(nn.View(-1):setNumInputDims(3))
         model:add(nn.Linear(1024,1000))
         model:add(nn.LogSoftMax())

         model.imageSize = 256
         model.imageCrop = 224

         return model:cuda()
      end
    #+END_SRC
*** TODO resnet torch 
    SCHEDULED: <2016-09-19 Mon>
    - [[https://github.com/szagoruyko/wide-residual-networks][resnet git repository Sergey Zagoruyko]]
*** [[https://github.com/szagoruyko/cifar.torch][cifar.torch]]
**** [[https://github.com/szagoruyko/cifar.torch][git repository]]
**** [[http://torch.ch/blog/2015/07/30/cifar.html][blog]]
*** preprocess image data
**** reference
***** [[https://github.com/torch/tutorials/blob/master/2_supervised/1_data.lua][tutorial]]
**** load data
     #+BEGIN_SRC lua
       ----------------------------------------------------------------------
       print '==> loading dataset'

       -- We load the dataset from disk, and re-arrange it to be compatible
       -- with Torch's representation. Matlab uses a column-major representation,
       -- Torch is row-major, so we just have to transpose the data.

       -- Note: the data, in X, is 4-d: the 1st dim indexes the samples, the 2nd
       -- dim indexes the color channels (RGB), and the last two dims index the
       -- height and width of the samples.

       loaded = torch.load(train_file,'ascii')
       trainData = {
          data = loaded.X:transpose(3,4),
          labels = loaded.y[1],
          size = function() return trsize end
       }

     #+END_SRC
**** image size scale
     #+BEGIN_SRC lua 
       require 'image'
       image_name = paths.basename('Goldfish3.jpg')
       print(image_name)
       im = image.load(image_name)
       im = image.scale(im, 224, 224):double()
       itorch.image(im)
       im = torch.reshape(im, 1, 3, 224, 224)
       itorch.image(im)
       print(im:size())
     #+END_SRC
**** rescale and normalize the image globally
     #+BEGIN_SRC lua
       -- Rescales and normalizes the image
       function preprocess(im, img_mean)
         -- rescale the image
         local im3 = image.scale(im,224,224,'bilinear')
         -- subtract imagenet mean and divide by std
         for i=1,3 do im3[i]:add(-img_mean.mean[i]):div(img_mean.std[i]) end
         return im3
       end

       I = preprocess(im, net.transform):float()
       itorch.image(I)
     #+END_SRC
**** colorspace RGB -> YUV
     #+BEGIN_SRC lua
       print '==> preprocessing data: colorspace RGB -> YUV'
       for i = 1,trainData:size() do
          trainData.data[i] = image.rgb2yuv(trainData.data[i])
       end
       for i = 1,testData:size() do
          testData.data[i] = image.rgb2yuv(testData.data[i])
       end
     #+END_SRC
**** normalize all three channel locally
     #+BEGIN_SRC lua
       -- Local normalization
       print '==> preprocessing data: normalize all three channels locally'

       -- Define the normalization neighborhood:
       neighborhood = image.gaussian1D(13)

       -- Define our local normalization operator (It is an actual nn module, 
       -- which could be inserted into a trainable model):
       normalization = nn.SpatialContrastiveNormalization(1, neighborhood, 1):float()

       -- Normalize all channels locally:
       for c in ipairs(channels) do
          for i = 1,trainData:size() do
             trainData.data[{ i,{c},{},{} }] = normalization:forward(trainData.data[{ i,{c},{},{} }])
          end
          for i = 1,testData:size() do
             testData.data[{ i,{c},{},{} }] = normalization:forward(testData.data[{ i,{c},{},{} }])
          end
       end
     #+END_SRC
**** verify statistics
     #+BEGIN_SRC lua
       print '==> verify statistics'

       -- It's always good practice to verify that data is properly
       -- normalized.

       for i,channel in ipairs(channels) do
          trainMean = trainData.data[{ {},i }]:mean()
          trainStd = trainData.data[{ {},i }]:std()

          testMean = testData.data[{ {},i }]:mean()
          testStd = testData.data[{ {},i }]:std()

          print('training data, '..channel..'-channel, mean: ' .. trainMean)
          print('training data, '..channel..'-channel, standard deviation: ' .. trainStd)

          print('test data, '..channel..'-channel, mean: ' .. testMean)
          print('test data, '..channel..'-channel, standard deviation: ' .. testStd)
       end
     #+END_SRC
**** visualizing data
     #+BEGIN_SRC lua
       print '==> visualizing data'

       -- Visualization is quite easy, using itorch.image().

       if opt.visualize then
          if itorch then
          first256Samples_y = trainData.data[{ {1,256},1 }]
          first256Samples_u = trainData.data[{ {1,256},2 }]
          first256Samples_v = trainData.data[{ {1,256},3 }]
          itorch.image(first256Samples_y)
          itorch.image(first256Samples_u)
          itorch.image(first256Samples_v)
          else
             print("For visualization, run this script in an itorch notebook")
          end
       end
     #+END_SRC
**** one method used for =cifar= dataset [[https://github.com/szagoruyko/cifar.torch/blob/master/provider.lua][(code)]] 
     - RGB -> YUV
     - normalize Y channel locally
     - normalize U, V channel globally
       #+BEGIN_SRC lua
          -- preprocess trainSet
           local normalization = nn.SpatialContrastiveNormalization(1, image.gaussian1D(7))
           for i = 1,trainData:size() do
              xlua.progress(i, trainData:size())
              -- rgb -> yuv
              local rgb = trainData.data[i]
              local yuv = image.rgb2yuv(rgb)
              -- normalize y locally:
              yuv[1] = normalization(yuv[{{1}}])
              trainData.data[i] = yuv
           end
           -- normalize u globally:
           local mean_u = trainData.data:select(2,2):mean()
           local std_u = trainData.data:select(2,2):std()
           trainData.data:select(2,2):add(-mean_u)
           trainData.data:select(2,2):div(std_u)
           -- normalize v globally:
           local mean_v = trainData.data:select(2,3):mean()
           local std_v = trainData.data:select(2,3):std()
           trainData.data:select(2,3):add(-mean_v)
           trainData.data:select(2,3):div(std_v)
       #+END_SRC
*** Tensor note
**** =Tensor:t()=
     #+BEGIN_SRC emacs-lisp :results none
       (setq org-babel-lua-command "lua5.1")
     #+END_SRC


     #+BEGIN_SRC lua :results output
       torch = require('torch')

       x = torch.Tensor(2,3):fill(1)

       -- x is contiguous, so y points to the same thing
       y = x:contiguous():fill(2)

       print(x)
       print(y)

       -- x:t() is not contiguous, so z is a clone
       z = x:t():contiguous():fill(3.14)

       print(x)
       print(z)
     #+END_SRC

     #+RESULTS:
**** =index(dim, index)=
     #+BEGIN_SRC lua :results output :interpreter /usr/local/bin/lua
       -- [[
       Returns a new Tensor which indexes the given tensor along dimension dim and using the entries in torch.LongTensor index. The returned tensor has the same number of dimensions as the original tensor. The returned tensor does not use the same storage as the original tensor.
       ]]

       torch = require 'torch'
       x = torch.rand(5, 5)
       print(x)

       y = x:index(1, torch.LongTensor{3, 1})
       print(y)

       y:fill(1)

       print(y)
       print(x)
     #+END_SRC

** Lua
*** =__index= metamethod
    当你通过键来访问 =table= 的时候，如果这个键没有值，那么 =Lua= 就会
    寻找该 =table= 的 =metatable= （假定有 =metatable= ）中的 =__index= 键。
    如果 =__index= 包含一个表格， =Lua= 会在表格中查找相应的键。 
    #+BEGIN_SRC lua :results output
      other = { foo = 3 }
      t = setmetatable({}, { __index = other })
      print(t.foo)

      print(t.bar)
    #+END_SRC

    #+RESULTS:
    : 3
    : nil

    如果 =__index= 包含一个函数的话， =Lua= 就会调用那个函数， =table=
    和键会作为参数传递给函数。 =__index= 元方法查看表中元素是否存在，如果
    不存在，返回结果为 =nil= ；如果存在则由 =__index= 返回结果。
    #+BEGIN_SRC lua :results output
      mytable = setmetatable({key1 = "value1"}, {
        __index = function(mytable, key)
          if key == "key2" then
            return "metatablevalue"
          else
            return nil
          end
        end
      })

      print('hello world')
      print(mytable.key1,mytable.key2)
    #+END_SRC

    #+RESULTS:
    : hello world
    : value1	metatablevalue

    1. 在表中查找，如果找到，返回该元素，找不到则继续
    2. 判断该表是否有元表，如果没有元表，返回 =nil= ，有元表则继续。
    3. 判断元表有没有 =__index= 方法，如果 =__index= 方法为 =nil= ，则返回 =nil= ；如
       果 =__index= 方法是一个表，则重复1、2、3；如果 =__index= 方法是一个函数，
       则返回该函数的返回值。
*** =__newindex= 
    =__newindex= 元方法用来对表更新 =__newindex= 则用来对表访问 。
    当你给表的一个缺少的索引赋值，解释器就会查找 =__newindex= 元方法：如
    果存在则调用这个函数而不进行赋值操作。 
    #+BEGIN_SRC lua :results output
      mymetatable = {}
      mytable = setmetatable({key1 = "value1"}, { __newindex = mymetatable })

      print(mytable.key1)

      mytable.newkey = "新值2"
      print(mytable.newkey,mymetatable.newkey)

      mytable.key1 = "新值1"
      print(mytable.key1,mymetatable.key1)
    #+END_SRC

    #+RESULTS:
    : value1
    : nil	新值2
    : 新值1	nil


    以上实例中表设置了元方法 =__newindex= ，在对新索引键（newkey）赋值时
    （mytable.newkey = "新值2"），会调用元方法，而不进行赋值。而如果对
    已存在的索引键（key1），则会进行赋值，而不调用元方法 =__newindex=
    。 
    #+RESULTS:
    : new value	"4"

    #+BEGIN_SRC lua :results output
      mytable = setmetatable({key1 = "value1"}, {
        __newindex = function(mytable, key, value)
              rawset(mytable, key, "\""..value.."\"")

        end
      })

      mytable.key1 = "new value"
      mytable.key2 = 4

      print(mytable.key1,mytable.key2)
    #+END_SRC
*** iterate =string=
    #+BEGIN_SRC lua :results output
      a = 'fds.fd.ds'

      for char in a:gmatch"." do
         print(char)
      end
    #+END_SRC

    #+RESULTS:
    : f
    : d
    : s
    : .
    : f
    : d
    : .
    : d
    : s
*** [[http://www.newthinktank.com/2015/06/learn-lua-one-video/][one video tutorial for lua]]
** Git
*** git stash
**** TODO [[https://git-scm.com/book/zh/v1/Git-%25E5%25B7%25A5%25E5%2585%25B7-%25E5%2582%25A8%25E8%2597%258F%25EF%25BC%2588Stashing%25EF%25BC%2589][tutorial]]
** C++
*** TODO [[/Users/zhangli/Documents/Library.papers3/Files/2A/2A75498D-39F3-4E24-A6F0-5CE79A0A5A11.pdf][C++ concurrency in action]]
    SCHEDULED: <2016-09-06 二>
**** [[https://www.gitbook.com/book/chenxiaowei/cpp_concurrency_in_action/details][resource for book]]
** Alfred
*** [[http://www.alfredworkflow.com/][workflow repository]]
** Latex
*** [[http://latex.wikia.com/wiki/List_of_LaTeX_symbols][reference for symbol]]
