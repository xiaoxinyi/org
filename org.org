* Tutorial
** Paper Reading
*** Deep learning
**** general paper
***** TODO [[/Users/zhangli/Documents/Library.papers3/Files/D6/D6FD20F2-226C-49D9-B4DB-FF1AF8C9C987.pdf][Caffe: Convolutional Architecture for Fast Feature Embedding]]
      SCHEDULED: <2016-08-24 Wed>
***** TODO [[/Users/zhangli/Documents/Library.papers3/Files/73/7398D9FD-507C-42B8-A5C1-07CABA329B0D.pdf][Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift]][[[http://arxiv.org/abs/1502.03167][arxiv]]] 
***** DONE [[/Users/zhangli/Documents/Library.papers3/Files/27/27F7C527-407C-4EF2-A337-350046B64813.pdf][CNN Features off-the-shelf: an Astounding Baseline for Recognition]] [[[http://arxiv.org/pdf/1403.6382v3.pdf][arxiv]]]
      CLOSED: [2016-10-26 Wed 17:12]
**** Localization and object detection
***** TODO [[/Users/zhangli/Documents/Library.papers3/Files/08/08D6B019-A046-46B3-B04D-07ED58F12367.pdf][OverFeat:Integrated Recognition, Localization and Detection using Convolutional Networks]]
      SCHEDULED: <2016-10-26 Wed>
***** TODO [[/Users/zhangli/Documents/Library.papers3/Files/C0/C0C326C2-9D41-41B1-9384-D98592C664FA.pdf][What makes for effective detection proposals?]]［[[https://arxiv.org/pdf/1502.05082v3.pdf][arxiv]]］
      SCHEDULED: <2016-10-27 Thu>
***** TODO [[https://arxiv.org/pdf/1608.08021v1.pdf][PVANET: Deep but Lightweight Neural Networks for Real-time Object Detection]]
      SCHEDULED: <2016-10-27 Thu>
***** TODO [[/Users/zhangli/Documents/Library.papers3/Files/1B/1B6E4A83-D59E-403D-B84A-C3DB8FFF3760.pdf][You Only Look Once: Unified, Real-Time Object Detection]] [[[https://arxiv.org/pdf/1506.02640v5.pdf][arxiv]]]
** Linux Note
*** sed
#+BEGIN_SRC sh
  # from line 90 to line 100
  sed -n '90, 100p' file.txt

  # line 100
  sed -n '100, 1p' file.txt

  # from line a to line b
  sed -n 'a, bp' file.txt

  # if a > b return line a
#+END_SRC
*** nvidia installation
#+BEGIN_SRC sh
sudo add-apt-repository ppa:graphics-drivers/ppa

sudo apt-get update && sudo apt-get install nvidia-355
#+END_SRC
*** apt-get list installed packages
#+BEGIN_SRC sh
apt list --installed
#+END_SRC
*** recent installed packages
    #+BEGIN_SRC sh
    grep " install " /var/log/dpkg.log
    #+END_SRC
*** list all users
#+BEGIN_SRC sh
  sed 's/:.*//' /etc/passwd
#+END_SRC
*** list all usergroup =groups=
*** usermod
**** add =newuser= to group =staff=
#+BEGIN_SRC sh
sudo usermod -G staff newuser
#+END_SRC
**** modify =newuser= to =newuser1=
#+BEGIN_SRC sh
sudo usermod -l newsuer1 newuser
#+END_SRC
*** watch GPU
#+BEGIN_SRC sh
watch -n 0.5 nvidia-smi
#+END_SRC
*** install =nvidia= driver
#+BEGIN_SRC sh
sudo add-apt-repository ppa:graphics-drivers/ppa

sudo apt-get update && sudo apt-get install nvidia-355
#+END_SRC
*** inspect symbol in =.so=
    #+BEGIN_SRC sh
    objdump -tT libName.so | grep symbol
    #+END_SRC
*** [[http://blog.csdn.net/wooin/article/details/580679][ =ldconfig= ]]
*** kill
**** [[http://www.thegeekstuff.com/2009/12/4-ways-to-kill-a-process-kill-killall-pkill-xkill/][documentation]]
*** find
**** or
     #+BEGIN_SRC sh
       find Documents \( -name "*.py" -or -name "*.html" \)
     #+END_SRC
** Tmux Note
*** =tmux= plugin installation
    Add plugin to the list of TPM plugins in =.tmux.conf=:
#+BEGIN_SRC sh
  set -g @plugin 'tmux-plugins/tmux-pain-control'
#+END_SRC
Hit =prefix + I= to fetch the plugin and source it.
*** move panel to a new window
    =prefix + : break-panel=
** Learning resource
*** [[https://github.com/sindresorhus/awesome][awesome series git]]
** Deep learning Note
*** deep learning resource
**** [[https://github.com/ChristosChristofidis/awesome-deep-learning][awesome deep learning git]]
**** [[https://github.com/jbhuang0604/awesome-computer-vision][awesome computer vision git]]
**** [[https://www.quora.com/How-do-I-learn-deep-learning-in-2-months][deep learning resource from quora]]
**** TODO [[https://www.youtube.com/playlist?list=PLrAXtmErZgOfMuxkACrYnD2fTgbzk2THW][Deep Learning School 2016]]
     SCHEDULED: <2016-10-04 Tue>
**** [[http://mp.weixin.qq.com/s?__biz=MjM5NDE4MTc2OA==&mid=2447696429&idx=1&sn=ceb6a5faf7c58fb814894f84c01e979d&mpshare=1&scene=1&srcid=1009zhC5J9YV0EhlGHtBISM2#rd][awesome series for machine learning]]
**** [[https://github.com/kjw0612/awesome-deep-vision][awesome-deep-vision]]
*** TODO [[http://karpathy.github.io/neuralnets/][Karpathy cnn tutorial]]
   SCHEDULED: <2016-08-21 Sun>
*** DONE [[http://karpathy.github.io/2015/05/21/rnn-effectiveness/][RNN tutorial]]
   CLOSED: [2016-09-02 Fri 21:50] SCHEDULED: <2016-08-21 Sun>
*** TODO [[https://cs224d.stanford.edu/notebooks/vanishing_grad_example.html][vanishing gradient]]
   SCHEDULED: <2016-08-22 Mon>
*** softmax loss for one example
    \[p_k = \frac{e^{f_k}}{\sum_j e^{f_j}}\]
    \[L_i = -log(p_{y_i})\]
    \[\frac{\partial L_i}{\partial f_k} = 1 - I(y_i = k)\]

     Suppose the probabilities we computed were =p = [0.2, 0.3, 0.5]=,
    and that the correct class was the middle one (with probability
    0.3). According to this derivation the gradient on the scores
    would be =df = [0.2, -0.7, 0.5]=.
    
    #+BEGIN_SRC python
      dscores = probs
      dscores[range(num_examples),y] -= 1
      dscores /= num_examples

      dW = np.dot(X.T, dscores)
      db = np.sum(dscores, axis=0, keepdims=True)
      dW += reg*W # don't forget the regularization gradient
    #+END_SRC
*** TODO [[http://sebastianruder.com/optimizing-gradient-descent/][gradient descent]] 
    SCHEDULED: <2016-08-23 Tue>
**** [[https://www.quora.com/What-is-the-vanishing-gradient-problem][Quora answer]]
- Problem

Gradient based methods learn a parameter's value by understanding how
a small change in the parameter's value will affect the network's
output. If a change in the parameter's value causes very small change
in the network's output - the network just can't learn the parameter
effectively, which is a problem. 

This is exactly what's happening in the vanishing gradient problem --
the gradients of the network's output with respect to the parameters
in the early layers become extremely small. That's a fancy way of
saying that even a large change in the value of parameters for the
early layers doesn't have a big effect on the output. Let's try to
understand when and why does this problem happen. 

- Cause

Vanishing gradient problem depends on the choice of the activation
function. Many common activation functions (e.g sigmoid or tanh)
'squash' their input into a very small output range in a very
non-linear fashion. For example, sigmoid maps the real number line
onto a "small" range of [0, 1]. As a result, there are large regions
of the input space which are mapped to an extremely small range. In
these regions of the input space, even a large change in the input
will produce a small change in the output - hence the gradient is
small. 

This becomes much worse when we stack multiple layers of such
non-linearities on top of each other. For instance, first layer will
map a large input region to a smaller output region, which will be
mapped to an even smaller region by the second layer, which will be
mapped to an even smaller region by the third layer and so on. As a
result, even a large change in the parameters of the first layer
doesn't change the output much. 

We can avoid this problem by using activation functions which don't
have this property of 'squashing' the input space into a small
region. A popular choice is Rectified Linear Unit which maps   
x to max(0,x) .

Hopefully, this helps you understand the problem of vanishing
gradients. I'd also recommend reading along this iPython notebook
which does a small experiment to understand and visualize this
problem, as well as highlights the difference between the behavior of
sigmoid and rectified linear units. 
*** TODO [[http://colah.github.io/][deep learning blog (colah)]]
    SCHEDULED: <2016-08-25 Thu>
*** backward
**** [[https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html][backward gradient for batch normalization]]

       \[ y_i = BN_{r, \beta}(x_i) \] 
       \[ \mu_{}_{}_{}_\Beta \leftarrow \frac{1}{m}\sum_{i-1}^m x_i \]  // mini-batch mean
       \[ \sigma_{\Beta}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}(x_i - \mu_{\Beta})^2 \]  // mini-batch variance
       \[ \hat{x}_{i}  \leftarrow \frac{(x_i - \mu{}_{\Beta})^2}{\sqrt{\sigma_{\Beta}^2 + \epsilon }} \]  // normalize
       \[ y_i \leftarrow \gamma\hat{x}_i + \beta \]  // scale and shift
*** confusion matrix
**** [[http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/][tutorial]]
*** why dropout layer 
**** Dropout is a form of regularisation.[[https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning][(quora)]]
***** How does it work?
      It essentially forces an artificial neural network to learn multiple
      independent representations of the same data by alternately randomly
      disabling neurons in the learning phase. 
***** What is the effect of this?
      The effect of this is that neurons are prevented from co-adapting too
      much which makes overfitting less likely. 
***** Why does this happen?
      The reason that this works is comparable to why using the mean outputs
      of many separately trained neural networks to reduces overfitting. 
*** what we should care about in deep learning
**** One time setup
***** activation functions
****** sigmod function
******* Saturated neurons “kill” the gradients
******* Sigmoid outputs are not zero-centered
******* Consider what happens when the input to a neuron is always positive? What can we say about the gradients on w? Always all positive or all negative :(
(this is also why you want zero-mean data!)
******* exp() is a bit compute expensive
****** tanh(x)
******* Squashes numbers to range [-1,1]
******* zero centered (nice)
******* still kills gradients when saturated :(
****** ReLU (Rectified Linear Unit)
******* Does not saturate (in +region)
******* Converges much faster than
******* sigmoid/tanh in practice (e.g. 6x)
******* Very computationally efficient
******* Not zero-centered output
******* hint: what is the gradient when x < 0?
******* people like to initialize ReLU neurons with slightly positive biases (e.g. 0.01)
****** Leaky ReLU  f(x) = max(0.01x, x)
******* Does not saturate 
******* Computationally efficient Converges much faster than sigmoid/tanh in practice! (e.g. 6x)
******* will not “die”.
****** Parametric Rectifier (PReLU) f(x) = max(\alpha*x, x)
******* backprop into \alpha (parameter)
****** Exponential Linear Units (ELU)
******* \[ \begin{cases} x \ \ if\ x > 0 \\ \alpha (exp(x) - 1) \ \ if\ x \leq 0 \end{cases} \]
******* All benefits of ReLU
******* Does not die
******* Closer to zero mean outputs
******* Computation requires exp()
****** Maxout “Neuron” 
******* \[ max(w_1^{T}x + b_1, w_2^{T}x + b_2) \]
******* Does not have the basic form of dot product -> nonlinearity
******* Generalizes ReLU and Leaky ReLU
******* Linear Regime! Does not saturate! Does not die!
******* Problem: doubles the number of parameters/neuron :(
***** preprocessing
****** Data Augmentation
******* Change the pixels without changing the label Train on transformed data VERY widely used
******* Horizontal flips
******* Random crops/scales
******** Training: sample random crops / scales ResNet:
         1. Pick random L in range [256, 480]
         2. Resize training image, short side = L
         3. Sample random 224 x 224 patch
******** Testing: average a fixed set of crops
         1. Resize image at 5 scales: {224, 256, 384, 480, 640}
         2. For each size, use 10 224 x 224 crops: 4 corners +
            center, + flips
******** Color jitter
********* Simple
          1. Randomly jitter contrast
********* Complex
          1. Apply PCA to all [R, G, B] pixels in training set
          2. Sample a “color offset” along principal component directions
          3. Add offset to all pixels of a training image
******** Get creative! 
********* Random mix/combinations of :
          - translation
          - rotation
          - stretching
          - shearing
          - lens distortions, ... (go crazy)
******* Summary 
        - Simple to implement, use it
        - Especially useful for small datasets
        - Fits into framework of noise / marginalization
******** A general theme: 
         - Training: Add random noise
         - pTesting: Marginalize over the noise
***** weight initialization
****** Small random numbers (gaussian with zero mean and 1e-2 standard deviation)
       #+BEGIN_SRC python
         W = 0.01 * np.random.randn(D, H)
       #+END_SRC
******* Works ~okay for small networks, but can lead to non-homogeneous distributions of activations across the layers of a network.
******* 10-layer net with 500 neurons on each layer, using tanh non-linearities
******** All activations become zero!
******** Almost all neurons completely saturated, either -1 and 1. Gradients will be all zero.
****** Xavier initialization [Glorot et al., 2010]
       #+BEGIN_SRC python
         W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)
       #+END_SRC
******* Reasonable initialization
******* but when using the ReLU nonlinearity it breaks.
****** He et al., 2015
       #+BEGIN_SRC python
         W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in / 2)
       #+END_SRC
****** Proper initialization is an active area of research
******* Understanding the difficulty of training deep feedforward neural networks by Glorot and Bengio, 2010 
******* Exact solutions to the nonlinear dynamics of learning in deep linear neural networks by Saxe et al, 2013
******* Random walk initialization for training very deep feedforward networks by Sussillo and Abbott, 2014
******* Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification by He et al., 2015
******* Data-dependent Initializations of Convolutional Neural Networks by Krähenbühl et al., 2015
******* All you need is a good init, Mishkin and Matas, 2015
***** layer tricks
****** Batch Normalization
******* consider a batch of activations at some layer. To make each dimension unit gaussian, apply:
        \[ \hat{x}_{(k)}} = \frac{x^{(k)} - E[x^{(k)}]}{\sqrt{Var[x^{(k)}]}} \]
******* this is a vanilla differentiable function...
******* compute the empirical mean and variance independently for each dimension.
******* Normalize
******* Usually inserted after Fully Connected / (or Convolutional) layers, and before nonlinearity.
******* Problem: do we necessarily want a unit gaussian input to a tanh layer?
******* advantages
******** improves gradient flow through the network
******** Allows higher learning rates
******** Reduces the strong dependence on initialization
******** Acts as a form of regularization in a funny way, and slightly reduces the need for dropout, maybe
******* algorithm
        \[ y_i = BN_{r, \beta}(x_i) \] 
        \[ \mu_{}_{}_{}_\Beta \leftarrow \frac{1}{m}\sum_{i-1}^m x_i \]  // mini-batch mean
        \[ \sigma_{\Beta}^{2} \leftarrow \frac{1}{m} \sum_{i=1}^{m}(x_i - \mu_{\Beta})^2 \]  // mini-batch variance
        \[ \hat{x}_{i}  \leftarrow \frac{(x_i - \mu{}_{\Beta})^2}{\sqrt{\sigma_{\Beta}^2 + \epsilon }} \]  // normalize
        \[ y_i \leftarrow \gamma\hat{x}_i + \beta \]  // scale and shift
******* Note: at test time BatchNorm layer functions differently:
        - The mean/std are not computed based on the batch. Instead, a
          single fixed empirical mean of activations during training
          is used.
        - (e.g. can be estimated during training with running averages)
***** regularization
****** Dropout
       - randomly set some neurons to zero in the forward pass
         #+BEGIN_SRC python
           p = 0.5  # probability of keeping a unit active. high = less dropout

           def train_step(X):
               """ X contains the data """

               # forward pass for example 3-layer neural network
               H1 = np.maximum(0, np.dot(W1, X) + b1)
               U1 = np.random.rand(*H1.shape) < p  # first dropout mask
               H1 *= U1  # drop!
               H2 = np.maximum(0, np.dot(W2, H1) + b2)
               U2 = np.random.rand(*H2.shape) < p  # second dropout mask
               H2 *= U2  # drop!
               out = np.dot(W3, H2) + b3

               # backward pass: compute gradients... (not shown)
               # perform paramter update... (not shown)
         #+END_SRC
******* How could this possibly be a good idea?
        - Forces the network to have a redundant representation
        - Dropout is training a large ensemble of models (that share parameters).
        - Each binary mask is one model, gets trained on only ~one
          datapoint.
******* At test time...
        - Ideally: want to integrate out all the noise
        - Monte Carlo approximation: do many forward passes with
          different dropout masks, average all predictioins
        - Can in fact do this with a single forward pass! Leave all
          input neurons turned on (no dropout).
          + during test : a = W0*x + W1*y
          + during train:
            E[a] = 1/4*(W0*0 + W1*0 + W0*0 + W1*y + W0*x
            + W1*0 + W0*x + W1*y) = 1/4*(2W0*x + 2W1*y) = 1/2(W0*x +
              W1*y)
          + with p = 0.5, using all inputs in the forward pass would
            inflate the activations by 2x from what the network was
            "used to" during training! => Have to compensate by
            scaling the activations back down by 1/2.
          + At test time all neurons are active always => output at
            test time = expected output at training time
            #+BEGIN_SRC python
              def predict(X):
                  # ensembled forward pass
                  H1 = np.maximum(0, np.dot(W1, X) + b1) * p  # NOTE: scale the activations
                  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p  # NOTE: scale the activations
                  out = np.dot(W3, H2) + b3
            #+END_SRC
            
            #+BEGIN_SRC python
              p = 0.5  # probability of keeping a unit active. higher = less dropout

              def train_step(X):
                  # forward pass for example 3-layer neural network
                  H1 = np.maximum(0, np.dot(W1, X) + b1)
                  U1 = (np.random.rand(*H1.shape) < p) / p  # first dropout mask. Notice /p!
                  H1 *= U1  # drop!
                  H2 = np.maximum(0, np.dot(W22, H1) + b2)
                  U2 = (np.random.rand(*H2.shape) < p) / p  # second dropout mask. Notice /p!
                  H2 *= U2  # drop!
                  out = np.dot(W3, H2) + b3



              def predict(X):
                  # ensembled forward pass
                  H1 = np.maximum(0, np.dot(W1, X) + b1)  # no scaling necessary
                  H2 = np.maximum(0, np.dot(W2, H1) + b2)
                  out = np.dot(W3, H2) + b3
            #+END_SRC

***** gradient checking
**** Training dynamics
***** babysitting the learning process
****** Double check that the loss is reasonable
       #+BEGIN_SRC python
         import numpy as np


         def init_two_layer_model(input_size, hidden_size, output_size):
             # initialize a model
             model = {}
             model['W1'] = 0.0001 * np.random.randn(input_size, hidden_size)
             model['b1'] = np.zeros(hidden_size)
             model['W2'] = 0.0001 * np.random.randn(hidden_size, output_size)
             model['b2'] = np.zeros(output_size)
             return model


         def two_layer_net(X_train, model, y_train, r):
             ''' returns the loss and the gradient for all parameters '''
             pass


         model = init_two_layer_model(32*32*3, 50, 10)  # input_size, hidden_size, number of classes
         loss, grad = two_layer_net(X_train, model, y_train, 0.0)  # diable regularization

         # loss 2.3026121617 loss ~2.3 'correct' for 10 classes


         model = init_two_layer_model(32*32*3, 50, 10)  # input_size, hidden_size, number of classes
         loss, grad = two_layer_net(X_train, model, y_train, 1e3)  # crank up regularization

         # 3.06859716482 loss went up, good (sanity check)

       #+END_SRC
****** Make sure that you can overfit very small portion of the training data
       #+BEGIN_SRC python
         model = init_two_lay_model(32*32*3, 50, 10)
         trainer = ClassifierTrainer()
         x_tiny = x_train[:20]  # take 20 examples
         y_tiny = y_train[:20]
         best_model, stats = trainer.train(x_tiny, y_tiny,
                                           model, two_layer_net,
                                           update='sgd', learning_rate_decay=1,
                                           sample_batches=False,
                                           learning_rate=1e-3, verbose=True)
       #+END_SRC
       - take the first 20 examples from CIFAR-10
       - turn off regularization (reg = 0.0)
       - use simple vanilla 'sgd'
       - very small loss, train accuracy 1.00, nice!
****** I like to start with small regularization and find learning rate that makes the loss go down.
       #+BEGIN_SRC python
         model = init_two_lay_model(32*32*3, 50, 10)
         trainer = ClassifierTrainer()
         x_tiny = x_train[:20]  # take 20 examples
         y_tiny = y_train[:20]
         best_model, stats = trainer.train(x_tiny, y_tiny,
                                           model, two_layer_net,
                                           update='sgd', learning_rate_decay=1,
                                           sample_batches=False,
                                           learning_rate=1e-6, verbose=True)

         # Loss barely changing: Learning rate is probably too low
         # Notice train/val accuracy goes up

       #+END_SRC
****** loss not going down: learning rate too low
****** loss exploding: learning rate too high
****** cost: NaN almost always means high learning rate...
****** Rough range for learning rate we should be cross-validating is somewhere [1e-3 ... 1e-5]
***** parameter updates
****** Mini-batch SGD
******* loop:
        1. Sample a batch of data
        2. Forward prop it through the graph, get loss
        3. Backprop to calculate the gradients
        4. Update the parameters using the gradient
           #+BEGIN_SRC python
             while True:
                 data_batch = dataset.sample_data_batch()
                 loss = network.forward(data_batch)
                 dx = network.backward()
                 x += - learning_rate * dx
           #+END_SRC
****** Momentum update
       #+BEGIN_SRC python
         # Gradient descent update
         x += - learning_rate * dx

         # Momentum update
         v = mu * v - learning_rate * dx  # integrate velocity
         x += v  # integrate position

       #+END_SRC
       - Physical intepretation as ball rolling down the loss
         function + friction (\mu coefficient)
       - \mu = usually ~ 0.5, 0.9, or 0.99 (sometimes anneled over time,
         e.g. from 0.5 -> 0.99)
       - Allows a velocity to build up along shallow directions
       - Velocity becomes damped in steep direction due to quickly
         changing sign
****** Nesterov Momentum update
       v_t = \mu v_{t-1} - \epsilon \nabla f(\theta_{t-1} + \mu v_{t-1})

       \theta_t = \theta_{t-1} + v_t 
       
       \theta_{t-1} + \mu v_{t-1} Slightly inconvenient usually we have : \theta_{t-1}
       \nabla f(\theta_{t-1})  
       - variable transform and rearranging saves the day: \phi_{t-1} = \theta_{t-1} + \mu v_{t-1}
       - replace all \theta  with \phi , rearrange and obtain:

         v_t = \mu v_{t-1} - \epsilon \nabla f(\phi_{t-1})

       \phi_t = \phi_{t-1} - \mu v_{t-1} + (1 + \mu)v_t 
       #+BEGIN_SRC python
         # Nesterov momentum update rewrite
         v_prev = v
         v = mu * v - learning_rate * dx
         x += -mu * v_prev + (1 + mu) * v
       #+END_SRC
****** AdaGrad update
       - Added element-wise scaling of the gradient based on the
         historical sum of squares in each dimension
         #+BEGIN_SRC python
           # Adagrad update

           cache += dx**2
           x += - learning_rate * dx / (np.sqrt(cache) + 1e-7)
         #+END_SRC
****** RMSProp update
       #+BEGIN_SRC python
         # Adagrad update
         cache += decay_rate * cache + (1 - decay_rate) * dx**2
         x += - learning_rate * dx / (np.sqrt(cache) + 1e-7)
       #+END_SRC
****** Adam update
       #+BEGIN_SRC python
         # Adam
         m = beta1*m + (1 - beta1)*dx  # update first moment momentum
         v = beta2*v + (1 - beta2)*(dx**2)  # update second moment RMSProp-like

         x += - learning_rate * m / (np.sqrt(v) + 1e-7)
       #+END_SRC

       #+BEGIN_SRC python
         # Adam
         m, v =  # ... initialize caches to zeros
         for t in xrange(1, big_number):
             dx =  # ... evaluate gradient
             m = beta1 * m + (1 - beta1) * dx  # update first moment
             v = beta2 * v + (1 - beta2) * (dx**2)  # update second moment
             mb = m / (1 - beta1**t)  # correct bias
             vb = v / (1 - beta2**t)  # correct bias
             x += - learning_rate * mb / (np.sqrt(vb) + 1e-7)
       #+END_SRC
       - The biaas correction compensates for the fact that m, v are
         initialize at zero and need some time to "warm up".
       - bias correction only relevant in first few iteration when t
         is small
****** learning rate as a hyperparameter => learning rate decay over time! 
       - step decay: e.g. decay learning rate by half every few epochs
       - exponential decay: \[ \alpha = \alpha_0 e^{-kt }\]
       - 1/t decay: \[ \alpha = \alpha_0 / (1 + kt) \]
****** Adam is a good default choice in most cases
***** hyperparameter optimization
****** network architecture
****** learning rate, its decay schedule, update type
****** regularization (L2/Droout strength)
****** monitor and visualize the loss curve
****** monitor and visualize the accuracy
       - big gap = overfitting => increase regularization strength?
       - no gap => increase model capacity?
****** Track the ratio of weight updates / weight magnitudes:
       #+BEGIN_SRC python
         # assume parameter vector W and its gradient vector dW
         param_scale = np.linalg.norm(W.ravel())
         update = -learning_rate*dW  # simple SGD update
         update_scale = np.linalg.norm(update.ravel())
         W += update  # the actual update
         print update_scale / param_scale  # want -1e-3
       #+END_SRC
       - atio between the values and updates: ~ 0.0002 / 0.02 = 0.01 (about okay)
       - want this to be somewhere around 0.001 or so
****** Cross-validation strategy
******* I like to do coarse -> fine cross-validation in stages
        - First stage: only a few epochs to get rough idea of what params work
        - Second stage: longer running time, finer search ... (repeat
          as necessary)
          #+BEGIN_SRC python
            max_count = 100

            # coarse search
            for count in xrange(max_count):
                # note it's best to optimize in log space!
                reg = 10**uniform(-5, 5)
                lr = 10**uniform(-3, -6)

            # fine search
            for count in xrange(max_count):
                reg = 10**uniform(-4, 0)
                lr = 10**uniform(-3, -4)
          #+END_SRC

**** Evaluation
***** model ensembles
      1. Train multiple independent models
      2. At test time average their results (Enjoy 2% extra performace)
      3. Fun Tips/Tricks:
         - can also get a small boost from averaging multiple model
           checkpoints of a single model.
         - keep track of (and use at test time) a running average
           parameter vector:
           #+BEGIN_SRC python
             while True:
                 data_batch = dataset.sample_data_batch()
                 loss = network.forward(data_batch)
                 dx = network.backward()
                 x += - learning_rate * dx
                 x_test = 0.9995*x_test + 0.005*x  # use for test set
           #+END_SRC
*** CNN Note
**** Case Study
***** ResNet
      - Batch Normalization after every CONV layer
      - Xavier/2 initialization from He et al.
      - SGD + Momentum (0.9)
      - Learning rate: 0.1, divided by 10 when validation error plateaus Mini-batch size 256
      - Weight decay of 1e-5 No dropout used
      - 2-3 weeks of training on 8 GPU machine
      - at runtime: faster than a VGGNet! (even though it has 8x more layers)
**** cnn summary
     - ConvNets stack CONV,POOL,FC layers
     - - Trend towards smaller filters and deeper architectures
     - Trend towards getting rid of POOL/FC layers (just CONV) -
       Typical architectures look like  [(CONV-RELU)*N-POOL?]*M-(FC-RELU)*K,SOFTMAX
     - where N is usually up to ~5, M is large, 0 <= K <= 2. but
       recent advances such as ResNet/GoogLeNet challenge this paradigmp
*** Deep learning frameworks
**** [[/Users/zhangli/Documents/Library.papers3/Files/05/05D0A096-FABE-4247-99AB-F567BFFFBA3A.pptx][comparison]]
**** recommendation
     - Feature extraction / finetuning existing models: Use Caffe
     - Complex uses of pretrained models: Use Lasagne or Torch
     - Write your own layers: Use Torch
     - Crazy RNNs: Use Theano or Tensorflow
     - Huge model, need model parallelism: Use TensorFlow
*** transfer learning
**** [[http://cs231n.github.io/transfer-learning/][tutorial]]
**** ConvNet as fixed feature extractor
     - It is important for performance that these codes are ReLUd
      (i.e. thresholded at zero) if they were also thresholded during
      the training of the ConvNet on ImageNet (as is usually the
      case). Once you extract the 4096-D codes for all images, train
      a linear classifier (e.g. Linear SVM or Softmax classifier) for
      the new dataset.
**** Fine-tuning the ConvNet 
     - This is motivated by the observation that the earlier features
       of a ConvNet contain more generic features (e.g. edge detectors
       or color blob detectors) that should be useful to many tasks,
       but later layers of the ConvNet becomes progressively more
       specific to the details of the classes contained in the
       original dataset.
     - Keeping in mind that ConvNet features are more generic in early
       layers and more original-dataset-specific in later layers, here
       are some common rules of thumb for navigating the 4 major
       scenarios: 
       1. New dataset is small and similar to original dataset. Since
          the data is small, it is not a good idea to fine-tune the
          ConvNet due to overfitting concerns. Since the data is
          similar to the original data, we expect higher-level
          features in the ConvNet to be relevant to this dataset as
          well. Hence, the best idea might be to train a linear
          classifier on the CNN codes.
       2. New dataset is large and similar to the original
          dataset. Since we have more data, we can have more
          confidence that we won’t overfit if we were to try to
          fine-tune through the full network.
       3. New dataset is small but very different from the original
          dataset. Since the data is small, it is likely best to only
          train a linear classifier. Since the dataset is very
          different, it might not be best to train the classifier form
          the top of the network, which contains more dataset-specific
          features. Instead, it might work better to train the SVM
          classifier from activations somewhere earlier in the
          network.
       4. New dataset is large and very different from the original
          dataset. Since the dataset is very large, we may expect that
          we can afford to train a ConvNet from scratch. However, in
          practice it is very often still beneficial to initialize
          with weights from a pretrained model. In this case, we would
          have enough data and confidence to fine-tune through the
          entire network. 
**** reference
***** [[/Users/zhangli/Documents/Library.papers3/Files/27/27F7C527-407C-4EF2-A337-350046B64813.pdf][CNN Features off-the-shelf: an Astounding Baseline for Recognition]] [[[http://arxiv.org/pdf/1403.6382v3.pdf][arxiv]]]
*** object detection
**** tutorial
***** TODO [[http://pjreddie.com/darknet/yolo/][yolo]]
      SCHEDULED: <2016-09-27 Tue>
****** references
******* [[/Users/zhangli/Documents/Library.papers3/Files/C0/C0EBD2D7-126A-4290-8AB6-AAA516E5A5DA.pdf][You Only Look Once: Unified, Real-Time Object Detection]] [[https://arxiv.org/pdf/1506.02640v5.pdf][(arxiv)]]
*** imbalance data for classification
**** [[https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set][quora answers]]
*** evaluation for classification
**** [[http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/][confusion matrix]]
**** [[http://www.dataschool.io/roc-curves-and-auc-explained/][ROC curve]]
ggggg** Caffe Note
*** [[https://github.com/BVLC/caffe/tree/85bb397acfd383a676c125c75d877642d6b39ff6/examples/feature_extraction][extract feature]]
**** using caffe to extract features
    #+BEGIN_SRC sh
      find `pwd`/examples/images -type f -exec echo {} \; > examples/_temp/temp.txt
      sed "s/$/ 0/" examples/_temp/temp.txt > examples/_temp/file_list.txt
      cd $CAFFE
      ./build/tools/extract_features models/bvlc_reference_caffenet/bvlc_reference examples/_temp/imagenet_val.prototxt example/_temp/feature fc7 10 lmdb GPU 0
    #+END_SRC
**** general command for extract feature using caffe
#+BEGIN_SRC sh
  extract_features pretrained_net_param  feature_extraction_proto_file \
  extract_feature_blob_name1[,name2,...]  save_feature_dataset_name1[,name2,...] \
  num_mini_batches  db_type  [CPU/GPU] [DEVICE_ID=0]
#+END_SRC
- 参数1是模型参数（.caffemodel）文件的路径。

- 参数2是描述网络结构的prototxt文件。程序会从参数1的caffemodel文件里找
  对应名称的layer读取参数。 

- 参数3是需要提取的blob名称，对应网络结构prototxt里的名称。blob名称可
  以有多个，用逗号分隔。每个blob提取出的特征会分开保存。 

- 参数4是保存提取出来的特征的数据库路径，可以有多个，和参数3中一一对应，
  以逗号分隔。如果用LMDB的话，路径必须是不存在的（已经存在的话要改名或
  者删除）。 


- 参数5是提取特征所需要执行的batch数量。这个数值和prototxt里DataLayer
  中的Caffe的DataLayer(或者ImageDataLayer)中的batch_size参数相乘，就是
  会被输入网络的总样本数。设置参数时需要确保batch_size *
  num_mini_batches等于需要提取特征的样本总数，否则提取的特征就会不够数
  或者是多了。 


- 参数6是保存特征使用的数据库类型，支持lmdb和leveldb两种(小写)。推荐使用
lmdb，因为lmdb的访问速度更快，还支持多进程同时读取。 

- 参数7决定使用GPU还是CPU，直接写对应的三个大写字母就行。省略不写的话默
认是CPU。 

- 参数8决定使用哪个GPU，在多GPU的机器上跑的时候需要指定。省略不写的话默
认使用0号GPU。 

注意事项
- 提取特征时，网络运行在Test模式下
    * Dropout层在Test模式下不起作用，不必担心dropout影响结果
    * Train和Test的参数写在同一个Prototxt里的时候，改参数的时候注意不
      要改错地方(比如有两个DataLayer的情况下) 
- 减去均值图像
    * 提取特征时，输入的图像要减去均值
    * 应该减去训练数据集的均值
- 提取哪一层
    * 不要提取Softmax网络的最后一层(如AlexNet的fc8)，因为最后一层已经
      是分类任务的输出，作为特征的可推广性不够好
**** read from lmdb
    #+BEGIN_SRC pytho
      import numpy as np
      import caffe
      import lmdb
      from caffe.proto import caffe_pb2

      fea_lmdb = lmdb.open('featureA')
      lmdb_txn = fea_lmdb.begin()
      lmdb_cursor = lmdb_txn.cursor()
      features = []

      for key, value in lmdb_cursor:
          datum = caffe_pb2.Datum()
          datum.ParseFromString(value)
          data = caffe.io.datum_to_array(datum)
          features.append(data)

    #+END_SRC
**** image recognition using =cos= similarity measure
#+BEGIN_SRC python

  import numpy as np
  import caffe
  import lmdb
  from caffe.proto import caffe_pb2
  from scipy import spatial


  # 3 steps to read form lmdb
  fea_lmdb = lmdb.ope




n('/root/caffe/examples/_temp/featureA')
  lmdb_txn = fea_lmdb.begin()
  lmdb_cursor = lmdb_txn.cursor()
  features = []

  for key, value in lmdb_cursor:
      datum = caffe_pb2.Datum()
      # Parse from serialized data
      datum.ParseFromString(value)
      data = caffe.io.datum_to_array(datum)
      features.append(data)

  out = []
  for f in features:
      out.append(f.flatten())

  n = len(out)
  similarity = np.zeros((n, n), dtype=np.double)

  for i in xrange(n):
      for j in xrange(n):
        # cosin distance
          similarity[i, j] = 1 - spatial.distance.cosine(out[i], out[j])

#+END_SRC
**** =cos= similarity result
- accuracy (true ture) : 53 / 55
#+BEGIN_SRC python
a = similarity[0:10, 0:10]
  array([[ 1.        ,  0.63231419,  0.84345085,  0.73587363,  0.58211244,
           0.67306891,  0.46881317,  0.56938226,  0.65432654,  0.55240935],
         [ 0.63231419,  1.        ,  0.68508232,  0.56741804,  0.74116358,
           0.81706845,  0.71951714,  0.75391089,  0.78529276,  0.74174079],
         [ 0.84345085,  0.68508232,  1.        ,  0.78416825,  0.61635946,
           0.72695667,  0.54473343,  0.60050371,  0.70046374,  0.58715887],
         [ 0.73587363,  0.56741804,  0.78416825,  1.        ,  0.50801387,
           0.60814318,  0.5046651 ,  0.52948304,  0.68054069,  0.49502061],
         [ 0.58211244,  0.74116358,  0.61635946,  0.50801387,  1.        ,
           0.88589477,  0.56183335,  0.72687896,  0.60917844,  0.87135289],
         [ 0.67306891,  0.81706845,  0.72695667,  0.60814318,  0.88589477,
           1.        ,  0.63597132,  0.76000156,  0.7042399 ,  0.87401555],
         [ 0.46881317,  0.71951714,  0.54473343,  0.5046651 ,  0.56183335,
           0.63597132,  1.        ,  0.58212342,  0.64319046,  0.6254508 ],
         [ 0.56938226,  0.75391089,  0.60050371,  0.52948304,  0.72687896,
           0.76000156,  0.58212342,  1.        ,  0.74652927,  0.72233884],
         [ 0.65432654,  0.78529276,  0.70046374,  0.68054069,  0.60917844,
           0.7042399 ,  0.64319046,  0.74652927,  1.        ,  0.61672591],
         [ 0.55240935,  0.74174079,  0.58715887,  0.49502061,  0.87135289,
           0.87401555,  0.6254508 ,  0.72233884,  0.61672591,  1.        ]])

np.sum(a > 0.5)
96
#+END_SRC
- false true : 2 / 100
#+BEGIN_SRC python
In [1]: ab = similarity[0:10, 10:]

In [2]: ab
Out[2]:
array([[ 0.2842583 ,  0.37596221,  0.27628312,  0.12041221,  0.29636999,
         0.13618284,  0.1381707 ,  0.17832465,  0.21937008,  0.40752771],
       [ 0.32961919,  0.49064045,  0.29595205,  0.093565  ,  0.39657901,
         0.17370467,  0.15514055,  0.2672414 ,  0.31652746,  0.46922921],
       [ 0.31926577,  0.45413662,  0.26234978,  0.1560283 ,  0.30816957,
         0.15273065,  0.16850629,  0.22604249,  0.25764858,  0.44164225],
       [ 0.26623039,  0.3611369 ,  0.20121232,  0.11351721,  0.21726182,
         0.11916629,  0.1431136 ,  0.20710409,  0.22387793,  0.31652456],
       [ 0.30927462,  0.35910132,  0.2650208 ,  0.08663475,  0.37263798,
         0.10722143,  0.09815253,  0.17950735,  0.20988739,  0.50689106],
       [ 0.32089366,  0.40492257,  0.28595893,  0.09466663,  0.37709065,
         0.10737807,  0.10595637,  0.19340299,  0.23139416,  0.51704389],
       [ 0.29795872,  0.3890121 ,  0.26349005,  0.08589599,  0.36945176,
         0.16923292,  0.11844475,  0.24970864,  0.31689723,  0.36337912],
       [ 0.28911623,  0.33516171,  0.30897566,  0.12046317,  0.36436887,
         0.10022814,  0.14957088,  0.29092572,  0.3343103 ,  0.47673998],
       [ 0.31926479,  0.43550698,  0.31588098,  0.09185497,  0.33737191,
         0.15741605,  0.16819127,  0.34134218,  0.38785466,  0.41883917],
       [ 0.29190126,  0.3130953 ,  0.25801771,  0.07097081,  0.34608239,
         0.09577894,  0.0842366 ,  0.14185045,  0.19112799,  0.47368384]])

In [3]: np.sum(ab > 0.5)
Out[3]: 2

#+END_SRC
*** Autoencoders
**** TODO [[http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/][UFLDL]]
     SCHEDULED: <2016-08-25 Thu>
*** TODO [[https://github.com/soumith/convnet-benchmarks][convnet benchmarks]]
    SCHEDULED: <2016-09-09 Fri>
** Emacs Notes
*** =elpy=
**** use-package =elpy=
#+BEGIN_SRC lisp
  ;; bind return key with <RET>, must be capitalized
  (:bind (M-<RET> . elpy-shell-send-current-statement))
#+END_SRC
**** elpy send function definition =C-M-x=
*** =ac-c-headers= locate =c= headers
#+BEGIN_SRC sh
gcc -xc++ -E -v -
#+END_SRC
#+BEGIN_SRC elisp
  (add-to-list achead:include-directories "/usr/include")
#+END_SRC
*** [[http://tuhdo.github.io/][tutorial]]
*** etags
    #+BEGIN_SRC sh
      find . -name "*.cpp" -print -o -name "*.h" -print | etags -  
    #+END_SRC
*** =babel-language=
    - directory =~/.emacs.d/elpa/org-20160620=
    - add =ob-lang.el=
*** =query-replace-regexp= [[https://www.gnu.org/software/emacs/manual/html_node/emacs/Regexp-Replace.html][note]]
*** =impatient-mode=
    #+BEGIN_SRC elisp
      ;; impatient-mode
      ;; useage: httpd start impatient-mode
      ;; localhost:8080/imp
      (use-package impatient-mode
        :ensure t
        :config
        (require 'impatient-mode))
    #+END_SRC
*** change etags to ctags
**** install =eproject= =etags-select=
     #+BEGIN_SRC elisp

       ;; eproject
       (use-package eproject
         :ensure t)

       (use-package etags-select
         :ensure t
         :config
         (defun build-ctags ()
           (interactive)
           (message "building project tags")
           (let ((root (eproject-root)))
             (shell-command (concat "ctags -e -R --extra=+fq --exclude=db --exclude=test --exclude=.git --exclude=public -f " root "TAGS " root))\
       )
           (visit-project-tags)
           (message "tags built successfully"))

         (defun visit-project-tags ()
           (interactive)
           (let ((tags-file (concat (eproject-root) "TAGS")))
             (visit-tags-table tags-file)
             (message (concat "Loaded " tags-file))))

         (defun my-find-tag ()
           (interactive)
           (if (file-exists-p (concat (eproject-root) "TAGS"))
               (visit-project-tags)
             (build-ctags))
           (etags-select-find-tag-at-point))

         (global-set-key (kbd "M-.") 'my-find-tag)

         )
     #+END_SRC
**** [[http://mattbriggs.net/blog/2012/03/18/awesome-emacs-plugins-ctags/][reference]]
*** =hs-minor-mode=
**** [[https://www.emacswiki.org/emacs/HideShow][reference]]
**** elisp code
     #+BEGIN_SRC elisp
       ;; hs-minor-mode
       (defun toggle-selective-display (column)
         (interactive "P")
         (set-selective-display
          (or column
              (unless selective-display
                (1+ (current-column))))))

       (defun toggle-hiding (column)
             (interactive "P")
             (if hs-minor-mode
                 (if (condition-case nil
                         (hs-toggle-hiding)
                       (error t))
                     (hs-show-all))
               (toggle-selective-display column)))

       (load-library "hideshow")
       (global-set-key (kbd "C-+") 'toggle-hiding)
       (global-set-key (kbd "C-\\") 'toggle-selective-display)

       (add-hook 'c-mode-common-hook   'hs-minor-mode)
       (add-hook 'emacs-lisp-mode-hook 'hs-minor-mode)
       ;; (add-hook 'elpy-mode-hook 'hs-minor-mode)
       (add-hook 'lua-mode-hook 'hs-minor-mode)

     #+END_SRC
*** install on =OSX=
    #+BEGIN_SRC sh
      brew install --with-cocoa --srgb --with-x  emacs
      brew linkapps emacs
    #+END_SRC
*** others' emacs configuration
**** TODO [[https://github.com/sachac/.emacs.d/blob/gh-pages/Sacha.org][Sacha Chua’s Emacs configuration]]
**** TODO [[https://github.com/danielmai/.emacs.d/blob/master/config.org][Danielmai's]]
**** [[https://github.com/hrs/dotfiles/blob/master/emacs.d/configuration.org][hrs]]
**** [[https://github.com/mwfogleman/config/blob/master/home/.emacs.d/michael.org][micheal]]
** =Org-mode= Note
*** move item up/down =M-up= =org-metaup=
*** change all level to next level =M-shift-left=
*** =C-x n s= (org-narrow-to-subtree)
*** =C-x n w= (widen)
*** literate programming in =org mode=
**** TODO [[http://www.howardism.org/Technical/Emacs/literate-devops.html][literate programming howard abrams]]
     SCHEDULED: <2016-09-13 Tue>
**** TODO [[http://www.howardism.org/Technical/LP/introduction.html][introduction Howard Abrams]]
     SCHEDULED: <2016-09-13 Tue>
*** show utf-8 character =C-c C-x \=
*** latex
**** [[http://orgmode.org/worg/org-tutorials/org-latex-preview.html][latex configuration]]
**** =C-c C-x C-l= latex preview in org
**** latex =tangle=
#+BEGIN_SRC latex :tangle example.tex
  \documentclass{article}

  \begin{document}

  \[
  e^{i\pi} = -1
  \]

  \[
  \int_0^\infty e^{-x^2} dx = \frac{\sqrt{\pi}}{2}
  \]

  \end{document}

#+END_SRC
*** org-narrow-forward
    #+BEGIN_SRC elisp
      ;; org-narrow-forward
      (defun zl/org-narrow-forward ()
        "Move to the next subtree at same level, and narrow to it."
        (interactive)
        (widen)
        (org-forward-heading-same-level 1)
        (org-narrow-to-subtree))

      (defun zl/set-org-keys ()
          (local-set-key "\C-xnm" 'zl/org-narrow-forward))

      (add-hook 'org-mode-hook 'zl/set-org-keys)
    #+END_SRC
*** babel-language
**** custom interpreter
#+begin_src emacs-lisp :results none
(setq org-babel-python-command "python3")
#+end_src

#+begin_src python :results output
import sys
print(sys.version)
#+end_src

#+RESULTS:
: 3.5.1 |Anaconda 4.0.0 (x86_64)| (default, Dec  7 2015, 11:24:55) 
: [GCC 4.2.1 (Apple Inc. build 5577)]

** Python Note
*** sample from an nd-array
#+BEGIN_SRC python
# sample from an nd-arrary
numpy.random.choice(range(vocab_size), p=p.ravel())
#+END_SRC
*** install package anaconda ubuntu
#+BEGIN_SRC sh
cd ~/anaconda2/bin
su
./pip install package
#+END_SRC
*** =sum= to keepdim
#+BEGIN_SRC python
np.sum(array, axis=0, keepdims=True)
#+END_SRC
*** =numpy.clip=
Clip (limit) the values in an array.

Given an interval, values outside the interval are clipped to the
interval edges. For example, if an interval of [0, 1] is specified,
values smaller than 0 become 0, and values larger than 1 become 1.

#+BEGIN_SRC python
>>> a = np.arange(10)
>>> np.clip(a, 1, 8)
array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8])
>>> a
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
>>> np.clip(a, 3, 6, out=a)
array([3, 3, 3, 3, 4, 5, 6, 6, 6, 6])
>>> a = np.arange(10)
>>> a
array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
>>> np.clip(a, [3,4,1,1,1,4,4,4,4,4], 8)
array([3, 4, 2, 3, 4, 5, 6, 7, 8, 8])
#+END_SRC
*** [[https://pyformat.info/][python string format]]
** Ipython Note
*** ipython config
    #+BEGIN_SRC sh
      ipython profile create

      emacs -nw /Users/zhangli/.ipython/profile_default/ipython_config.py
    #+END_SRC
    set =c.TerminalInteractiveShell.confirm_exit = False=
*** ipython share kernel
    #+BEGIN_SRC ipython
    %connect_info
    #+END_SRC
    #+BEGIN_SRC sh
    ipython console --existing /Users/zhangli/Library/Jupyter/runtime/kernel-0f76f3a7-104c-49dc-8942-162b50f5799b.json
    #+END_SRC
** Docker Note
*** docker installation
#+BEGIN_SRC sh
  wget -qO- https://get.docker.com/ | sh
  sudo usermod -aG docker sxwl1080
#+END_SRC
*** docker sourcelist
ubuntu: /etc/default/docker 
#+BEGIN_SRC example
DOCKER_OPTS="--dns 8.8.8.8 --dns 8.8.4.4 --insecure-registry dl.dockerpool.com:5000
#+END_SRC
*** docker push
    unauthorized: authentication required
    sudo docker login --username=xiaoxinyi
*** docker machine installation
#+BEGIN_SRC sh
  curl -L https://github.com/docker/machine/releases/download/v0.7.0/docker-machine-`uname -s`-`uname -m` > /usr/local/bin/docker-machine  
#+END_SRC
*** docker swarm
    - [[http://blog.arungupta.me/clustering-docker-swarm-techtip85/][tutorial]]
*** rm all exited containers
    #+BEGIN_SRC sh
      docker ps -a | grep Exited | cut -d ' ' -f 1 | xargs docker rm
    #+END_SRC
*** save and load image
    #+BEGIN_SRC sh
      # Get image id by doing:
      sudo docker images

      # Say you have image with id "matrix-data"

      # Save image with id:
      sudo docker save -o /home/matrix/matrix-data.tar matrix-data

      # Copy image from path to any host Now import to your local docker using :
      sudo docker load -i <path to copied image file>
    #+END_SRC
** Proxy
*** proxychains
#+BEGIN_SRC sh
sudo apt-get install -y proxychains
sudo cat "socks5  127.0.0.1 9999" >> /etc/proxychains.conf
ssh -p 1022  -fN -D 127.0.0.1:9999 root@192.168.199.1
proxychains curl www.google.co.jp
#+END_SRC
** Cuda Note
*** Configuring the kernel launch
kernel<<<grid of block, block of threads>>>(...)
square<<<dim3(bx,by,bz), dime(tx,ty,tz), sharem>>>(...)

grid of blocks : bx * by * bz
block of threads : tx * ty * tz
shared memory per block in bytes
*** Convert color to black and white
I = (R + G + B) / 3
I = .299f * R + .587f * G + .114f * B
*** [[http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#cuda-programming-model][ =nvcc= introduction]]
*** cs344 Note
- GPU is responsible for allocating blocks to SM(streaming multiprocessors)
- A block cannot run on more than one SM
- An SM may run more than one block
- All the SMs are running in parallel
- Threads in different block shouldn't cooperate
- Cuda make few guarantees about when and where thread blocks will run
- consequences
  + no assumptions blocks -> SM
  + no communication between blocks
- CUDA guarantees that:
  + all threads in a block run on the same SM at the same time
  + all blocks in a kernel finish before any blocks from next run
- threadIdx : thread within block threadIdx.x threadIdx.y
  + blockDim : size of block
  + blockIdx : block within grid
  + gridDim : size of grid
*** GPU memory model
[[./images/gpu-memory-model.png]]
  * All threads from a block can access the same variable in that
    block shared memory
  * Threads from two different blocks can access the same variable in
    global memory
  * Threads from different blocks have their own copy of local
    variables in local memory
  * Threads from the same block have their own copy of local variables
    in local memory

*** barrier
point in program where threads stop and wait. when all threads have
reached the barrier, they can proceed.
[[./images/synchronized.png]]
*** High-level strategies
1. Maximize arithmetic intensity
\[\frac{Math}{Memory}\]
  - maximize compute ops per thread
  - minimize time spent on memory per thread
     + move frequently-accessed data to fast memory
       local > shared >> global >> cpu memory
coalesce memeory
[[./images/coalesce.png]]
2. avoid thread divergence

*** =cudaMalloc=
    #+BEGIN_SRC c++
      float *device_data=NULL;  
      size_t size = 1024*sizeof(float);  
      cudaMalloc((void**)&device_data, size);  
    #+END_SRC
而device_data这个指针是存储在主存上的。之所以取device_data的地址，是为
了将cudaMalloc在显存上获得的数组首地址赋值给device_data。在函数中为形
参赋值是不会在实参中繁盛变化的，但是指针传递的是地址 

*** TODO [[/Users/zhangli/Documents/Library.papers3/Files/1E/1ED49076-5D40-4E5F-B232-918B17EA1596.pdf][What Every Programmer Should Know About Memory]]
    SCHEDULED: <2016-08-27 Sat>







*** levels of optimization
**** Picking good algorithms 3 - 10x
     - use mergesort \[O(nlgn)\] vs insertion sort \[O(n^2)\]
**** Basic principles for efficiency 3 - 10x
     - write cache-aware code. e.g. traverse rows vs cols
**** Arch-specific detailed optimizations 30% - 80%
     - block for the L1 cache
     - vector register SSE, AVX
**** \[\mu\]-optimization at instruction level
     - float recipe =sqrt = (float)0x5f3659da - (a >> 1)=
*** profiler
    - gprof
    - vtune
    - verysleepy
*** Amdahl's law
    - total speedup from parallelization is limited by protion of time
      spent doing some thing to be parralledized 
    \[ max speedup -> \frac{1}{1 - p}  \] p is % parallelizable time
*** most GPU codes are memory limited, always start by measuring bandwith
** Http
*** [[http://www.imooc.com/article/3582][http tutorial imooc]]
*** HTTP: Get & Post
Http协议定义了很多与服务器交互的方法，最基本的有4种，分别是
GET,POST,PUT,DELETE. 一个URL地址用于描述一个网络上的资源，而HTTP中的
GET, POST, PUT, DELETE就对应着对这个资源的查，改，增，删4个操作。 我们
最常见的就是GET和POST了。GET一般用于获取/查询资源信息，而POST一般用于
更新资源信息. 
- GET提交的数据会放在URL之后，以?分割URL和传输数据，参数之间以&相连，
  如EditPosts.aspx?name=test1&id=123456. POST方法是把提交的数据放在
  HTTP包的Body中.
- GET提交的数据大小有限制（因为浏览器对URL的长度有限制），而POST方法提
  交的数据没有限制.
- GET方式需要使用Request.QueryString来取得变量的值，而POST方式通过
  Request.Form来获取变量的值，也就是说Get是通过地址栏来传值，而Post是
  通过提交表单来传值。
- GET方式提交数据，会带来安全问题，比如一个登录页面，通过GET方式提交数
  据时，用户名和密码将出现在URL上，如果页面可以被缓存或者其他人可以访
  问这台机器，就可以从历史记录获得该用户的账号和密码. 

** OSX
*** lsof
#+BEGIN_SRC sh
  lsof -i TCP:port -n 
  lsof -i UDP:port -n
＃ listen ports
lsof -iTCP -sTCP:LISTEN -n -P
lsof -i -n -P | grep -i listen
#+END_SRC

#+RESULTS:
| COMMAND |  PID | USER    | FD  | TYPE |             DEVICE | SIZE/OFF | NODE | NAME    |          |
| Papers  |  448 | zhangli | 45u | IPv4 | 0x96671a7181200663 | 0t0      | TCP  | *:17320 | (LISTEN) |
| Papers  |  448 | zhangli | 47u | IPv6 | 0x96671a716df4a343 | 0t0      | TCP  | *:17320 | (LISTEN) |
| Alfred  | 1220 | zhangli | 4u  | IPv4 | 0x96671a71837b7663 | 0t0      | TCP  | *:57353 | (LISTEN) |
| Alfred  | 1220 | zhangli | 7u  | IPv6 | 0x96671a716df49883 | 0t0      | TCP  | *:57353 | (LISTEN) |
| Papers  |  448 | zhangli | 45u | IPv4 | 0x96671a7181200663 | 0t0      | TCP  | *:17320 | (LISTEN) |
| Papers  |  448 | zhangli | 47u | IPv6 | 0x96671a716df4a343 | 0t0      | TCP  | *:17320 | (LISTEN) |
| Alfred  | 1220 | zhangli | 4u  | IPv4 | 0x96671a71837b7663 | 0t0      | TCP  | *:57353 | (LISTEN) |
| Alfred  | 1220 | zhangli | 7u  | IPv6 | 0x96671a716df49883 | 0t0      | TCP  | *:57353 | (LISTEN) |

*** brew
    #+BEGIN_SRC sh :results output
      brew info imagemagick
    #+END_SRC

    #+RESULTS:
    #+begin_example
    imagemagick: stable 6.9.6-2 (bottled), HEAD
    Tools and libraries to manipulate images in many formats
    https://www.imagemagick.org/
    /usr/local/Cellar/imagemagick/6.9.1-10 (1,450 files, 17.5M)
      Poured from bottle on 2015-07-27 at 00:10:58
    /usr/local/Cellar/imagemagick/6.9.5-9_2 (1,468 files, 25M)
      Built from source on 2016-09-19 at 20:04:41 with: --with-x11
    /usr/local/Cellar/imagemagick/6.9.6-2 (1,469 files, 25M) *
      Built from source on 2016-10-31 at 10:43:46 with: --with-x11
    From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/imagemagick.rb
    ==> Dependencies
    Build: pkg-config
    Required: libtool, xz
    Recommended: jpeg, libpng, libtiff, freetype
    Optional: fontconfig, little-cms, little-cms2, libwmf, librsvg, liblqr, openexr, ghostscript, webp, openjpeg, fftw, pango
    ==> Requirements
    Optional: x11, perl >= 5.5
    ==> Options
    --with-fftw
        Compile with FFTW support
    --with-fontconfig
        Build with fontconfig support
    --with-ghostscript
        Build with ghostscript support
    --with-hdri
        Compile with HDRI support
    --with-liblqr
        Build with liblqr support
    --with-librsvg
        Build with librsvg support
    --with-libwmf
        Build with libwmf support
    --with-little-cms
        Build with little-cms support
    --with-little-cms2
        Build with little-cms2 support
    --with-opencl
        Compile with OpenCL support
    --with-openexr
        Build with openexr support
    --with-openjpeg
        Build with openjpeg support
    --with-openmp
        Compile with OpenMP support
    --with-pango
        Build with pango support
    --with-perl
        Compile with PerlMagick
    --with-quantum-depth-16
        Compile with a quantum depth of 16 bit
    --with-quantum-depth-32
        Compile with a quantum depth of 32 bit
    --with-quantum-depth-8
        Compile with a quantum depth of 8 bit
    --with-webp
        Build with webp support
    --with-x11
        Build with x11 support
    --with-zero-configuration
        Disables depending on XML configuration files
    --without-freetype
        Build without freetype support
    --without-jpeg
        Build without jpeg support
    --without-libpng
        Build without libpng support
    --without-libtiff
        Build without libtiff support
    --without-magick-plus-plus
        disable build/install of Magick++
    --without-modules
        Disable support for dynamically loadable modules
    --without-threads
        Disable threads support
    --HEAD
        Install HEAD version
#+end_example

*** =CR= =LF=
    The Carriage Return (CR) character (=0x0D=, =\r=) moves the cursor to
    the beginning of the line without advancing to the next line. This
    character is used as a new line character in Commodore and Early
    Macintosh operating systems (OS-9 and earlier). 

    The Line Feed (LF) character (=0x0A=, =\n=) moves the cursor down to
    the next line without returning to the beginning of the line. This
    character is used as a new line character in UNIX based systems
    (Linux, Mac OSX, etc) 

    The End of Line (EOL) sequence (0x0D 0x0A, =\r\n=) is actually two
    ASCII characters, a combination of the CR and LF characters. It
    moves the cursor both down to the next line and to the beginning
    of that line. This character is used as a new line character in
    most other non-Unix operating systems including Microsoft Windows,
    Symbian OS and others.
*** inspect file's encoding information
    #+BEGIN_SRC sh
      file -I org.org
    #+END_SRC

    #+RESULTS:
    : org.org: text/plain; charset=utf-8

** Torch
*** TODO [[https://github.com/torch/nn/blob/master/doc/training.md#stochasticgradientmodule-criterion][gradient in torch]]
    SCHEDULED: <2016-08-31 Wed>
*** JIT complier
    In the beginning, a compiler was responsible for turning a
    high-level language (defined as higher level than assembler) into
    object code (machine instructions), which would then be linked (by
    a linker) into an executable. 

    At one point in the evolution of languages, compilers would compile a
    high-level language into pseudo-code, which would then be interpreted
    (by an interpreter) to run your program. This eliminated the object
    code and executables, and allowed these languages to be portable to
    multiple operating systems and hardware platforms. Pascal (which
    compiled to P-Code) was one of the first; Java and C# are more recent
    examples. Eventually the term P-Code was replaced with bytecode, since
    most of the pseudo-operations are a byte long. 

    A Just-In-Time (JIT) compiler is a feature of the run-time
    interpreter, that instead of interpreting bytecode every time a method
    is invoked, will compile the bytecode into the machine code
    instructions of the running machine, and then invoke this object code
    instead. Ideally the efficiency of running object code will overcome
    the inefficiency of recompiling the program every time it runs. 
*** install =fblualib=
**** git clone [[https://github.com/facebook/fblualib][repository]]
**** =./install_all.sh=
**** =fb.debugger=
     #+BEGIN_SRC lua
     local debugger = require('fb.debugger')
     debugger.enter()
     #+END_SRC
**** TODO =fblualib= [[https://github.com/facebook/fblualib][git repository]]
     SCHEDULED: <2016-09-11 Sun>
*** [[https://github.com/torch/nngraph][ =nngraph= tutorial]]
*** TODO [[https://github.com/torch/demos][torch demos and tutorial]]
    SCHEDULED: <2016-09-12 Mon>
*** [[http://jucor.github.io/torch-doc-template/tensor.html][torch tensor reference]]
*** [[https://github.com/torch/torch7/wiki/Cheatsheet][torch cheatsheet]]
*** torch install packages
    #+BEGIN_SRC sh
      luarocks install torch-rocks install https://raw.github.com/andresy/mnist/master/rocks/mnist-scm-1.rockspec
    #+END_SRC
*** create a =nn= example
    #+BEGIN_SRC lua
      function createModel(nGPU)
         require 'cunn'

         local model = nn.Sequential()

         local function block(...)
            local arg = {...}
            local no = arg[2]
            model:add(nn.SpatialConvolution(...))
            model:add(nn.SpatialBatchNormalization(no,1e-3))
            model:add(nn.ReLU(true))
            model:add(nn.SpatialConvolution(no, no, 1, 1, 1, 1, 0, 0))
            model:add(nn.SpatialBatchNormalization(no,1e-3))
            model:add(nn.ReLU(true))
            model:add(nn.SpatialConvolution(no, no, 1, 1, 1, 1, 0, 0))
            model:add(nn.SpatialBatchNormalization(no,1e-3))
            model:add(nn.ReLU(true))
         end

         local function mp(...)
            model:add(nn.SpatialMaxPooling(...))
         end

         block(3, 96, 11, 11, 4, 4, 5, 5)
         mp(3, 3, 2, 2, 1, 1)
         block(96, 256, 5, 5, 1, 1, 2, 2)
         mp(3, 3, 2, 2, 1, 1)
         block(256, 384, 3, 3, 1, 1, 1, 1)
         mp(3, 3, 2, 2, 1, 1)
         block(384, 1024, 3, 3, 1, 1, 1, 1)

         model:add(nn.SpatialAveragePooling(7, 7, 1, 1))
         model:add(nn.View(-1):setNumInputDims(3))
         model:add(nn.Linear(1024,1000))
         model:add(nn.LogSoftMax())

         model.imageSize = 256
         model.imageCrop = 224

         return model:cuda()
      end
    #+END_SRC
*** TODO resnet torch 
    SCHEDULED: <2016-09-19 Mon>
**** [[https://github.com/szagoruyko/wide-residual-networks][resnet git repository Sergey Zagoruyko]]
**** TODO [[https://github.com/facebook/fb.resnet.torch/tree/master/pretrained][fb.resnet.torch]]
     SCHEDULED: <2016-09-11 Sun>
**** [[http://kaiminghe.com/icml16tutorial/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf][kaiming He tutorial]]
*** [[https://github.com/szagoruyko/cifar.torch][cifar.torch]]
**** [[https://github.com/szagoruyko/cifar.torch][git repository]]
**** [[http://torch.ch/blog/2015/07/30/cifar.html][blog]]
*** preprocess image data
**** reference
***** [[https://github.com/torch/tutorials/blob/master/2_supervised/1_data.lua][tutorial]]
**** load data
     #+BEGIN_SRC lua
       ----------------------------------------------------------------------
       print '==> loading dataset'

       -- We load the dataset from disk, and re-arrange it to be compatible
       -- with Torch's representation. Matlab uses a column-major representation,
       -- Torch is row-major, so we just have to transpose the data.

       -- Note: the data, in X, is 4-d: the 1st dim indexes the samples, the 2nd
       -- dim indexes the color channels (RGB), and the last two dims index the
       -- height and width of the samples.

       loaded = torch.load(train_file,'ascii')
       trainData = {
          data = loaded.X:transpose(3,4),
          labels = loaded.y[1],
          size = function() return trsize end
       }

     #+END_SRC
**** image size scale
     #+BEGIN_SRC lua 
       require 'image'
       image_name = paths.basename('Goldfish3.jpg')
       print(image_name)
       im = image.load(image_name)
       im = image.scale(im, 224, 224):double()
       itorch.image(im)
       im = torch.reshape(im, 1, 3, 224, 224)
       itorch.image(im)
       print(im:size())
     #+END_SRC
**** rescale and normalize the image globally
     #+BEGIN_SRC lua
       -- Rescales and normalizes the image
       function preprocess(im, img_mean)
         -- rescale the image
         local im3 = image.scale(im,224,224,'bilinear')
         -- subtract imagenet mean and divide by std
         for i=1,3 do im3[i]:add(-img_mean.mean[i]):div(img_mean.std[i]) end
         return im3
       end

       I = preprocess(im, net.transform):float()
       itorch.image(I)
     #+END_SRC
**** colorspace RGB -> YUV
     #+BEGIN_SRC lua
       print '==> preprocessing data: colorspace RGB -> YUV'
       for i = 1,trainData:size() do
          trainData.data[i] = image.rgb2yuv(trainData.data[i])
       end
       for i = 1,testData:size() do
          testData.data[i] = image.rgb2yuv(testData.data[i])
       end
     #+END_SRC
**** normalize all three channel locally
     #+BEGIN_SRC lua
       -- Local normalization
       print '==> preprocessing data: normalize all three channels locally'

       -- Define the normalization neighborhood:
       neighborhood = image.gaussian1D(13)

       -- Define our local normalization operator (It is an actual nn module, 
       -- which could be inserted into a trainable model):
       normalization = nn.SpatialContrastiveNormalization(1, neighborhood, 1):float()

       -- Normalize all channels locally:
       for c in ipairs(channels) do
          for i = 1,trainData:size() do
             trainData.data[{ i,{c},{},{} }] = normalization:forward(trainData.data[{ i,{c},{},{} }])
          end
          for i = 1,testData:size() do
             testData.data[{ i,{c},{},{} }] = normalization:forward(testData.data[{ i,{c},{},{} }])
          end
       end
     #+END_SRC
**** verify statistics
     #+BEGIN_SRC lua
       print '==> verify statistics'

       -- It's always good practice to verify that data is properly
       -- normalized.

       for i,channel in ipairs(channels) do
          trainMean = trainData.data[{ {},i }]:mean()
          trainStd = trainData.data[{ {},i }]:std()

          testMean = testData.data[{ {},i }]:mean()
          testStd = testData.data[{ {},i }]:std()

          print('training data, '..channel..'-channel, mean: ' .. trainMean)
          print('training data, '..channel..'-channel, standard deviation: ' .. trainStd)

          print('test data, '..channel..'-channel, mean: ' .. testMean)
          print('test data, '..channel..'-channel, standard deviation: ' .. testStd)
       end
     #+END_SRC
**** visualizing data
     #+BEGIN_SRC lua
       print '==> visualizing data'

       -- Visualization is quite easy, using itorch.image().

       if opt.visualize then
          if itorch then
          first256Samples_y = trainData.data[{ {1,256},1 }]
          first256Samples_u = trainData.data[{ {1,256},2 }]
          first256Samples_v = trainData.data[{ {1,256},3 }]
          itorch.image(first256Samples_y)
          itorch.image(first256Samples_u)
          itorch.image(first256Samples_v)
          else
             print("For visualization, run this script in an itorch notebook")
          end
       end
     #+END_SRC
**** one method used for =cifar= dataset [[https://github.com/szagoruyko/cifar.torch/blob/master/provider.lua][(code)]] 
     - RGB -> YUV
     - normalize Y channel locally
     - normalize U, V channel globally
       #+BEGIN_SRC lua
          -- preprocess trainSet
           local normalization = nn.SpatialContrastiveNormalization(1, image.gaussian1D(7))
           for i = 1,trainData:size() do
              xlua.progress(i, trainData:size())
              -- rgb -> yuv
              local rgb = trainData.data[i]
              local yuv = image.rgb2yuv(rgb)
              -- normalize y locally:
              yuv[1] = normalization(yuv[{{1}}])
              trainData.data[i] = yuv
           end
           -- normalize u globally:
           local mean_u = trainData.data:select(2,2):mean()
           local std_u = trainData.data:select(2,2):std()
           trainData.data:select(2,2):add(-mean_u)
           trainData.data:select(2,2):div(std_u)
           -- normalize v globally:
           local mean_v = trainData.data:select(2,3):mean()
           local std_v = trainData.data:select(2,3):std()
           trainData.data:select(2,3):add(-mean_v)
           trainData.data:select(2,3):div(std_v)
       #+END_SRC
**** tools
***** [[http://www.imagemagick.org/script/convert.php][convert tool]]
*** Tensor note
**** =Tensor:t()=
     #+BEGIN_SRC emacs-lisp :results none
       (setq org-babel-lua-command "th")
     #+END_SRC


     #+BEGIN_SRC lua :results output
       -- torch = require('torch')

       x = torch.Tensor(2,3):fill(1)

       -- x is contiguous, so y points to the same thing
       y = x:contiguous():fill(2)

       print(x)
       print(y)

       -- x:t() is not contiguous, so z is a clone
       z = x:t():contiguous():fill(3.14)

       print(x)
       print(z)
     #+END_SRC

     #+RESULTS:
     #+begin_example

       ______             __   |  Torch7                                         
      /_  __/__  ________/ /   |  Scientific computing for Lua. 
       / / / _ \/ __/ __/ _ \  |  Type ? for help                                
      /_/  \___/_/  \__/_//_/  |  https://github.com/torch         
                               |  http://torch.ch                  

                                                                           [0.0000s]	
                                                                           [0.0000s]	
                                                                           [0.0000s]	
                                                                           [0.0000s]	
                                                                           [0.0000s]	
                                                                           [0.0000s]	
                                                                           [0.0000s]	
      2  2  2
      2  2  2
     [torch.DoubleTensor of size 2x3]

                                                                           [0.0001s]	
      2  2  2
      2  2  2
     [torch.DoubleTensor of size 2x3]

                                                                           [0.0001s]	
                                                                           [0.0000s]	
                                                                           [0.0000s]	
                                                                           [0.0000s]	
                                                                           [0.0000s]	
      2  2  2
      2  2  2
     [torch.DoubleTensor of size 2x3]

                                                                           [0.0000s]	
      3.1400  3.1400
      3.1400  3.1400
      3.1400  3.1400
     [torch.DoubleTensor of size 3x2]

                                                                           [0.0001s]	
     Do you really want to exit ([y]/n)? 	
#+end_example

**** =index(dim, index)=
     #+BEGIN_SRC lua :results output 
       -- [[
       -- Returns a new Tensor which indexes the given tensor along dimension dim and using the entries in torch.LongTensor index. The returned tensor has the same number of dimensions as the original tensor. The returned tensor does not use the same storage as the original tensor.
       -- ]]

       -- torch = require 'torch'
       x = torch.rand(5, 5)
       print(x)

       y = x:index(1, torch.LongTensor{3, 1})
       print(y)

       y:fill(1)

       print(y)
       print(x)
     #+END_SRC

     #+RESULTS:
     #+begin_example

       ______             __   |  Torch7                                         
      /_  __/__  ________/ /   |  Scientific computing for Lua. 
       / / / _ \/ __/ __/ _ \  |  Type ? for help                                
      /_/  \___/_/  \__/_//_/  |  https://github.com/torch         
                               |  http://torch.ch                  

                                                                           [0.0000s]	
                                                                           [0.0000s]	
                                                                           [0.0000s]	
                                                                           [0.0000s]	
                                                                           [0.0000s]	
                                                                           [0.0000s]	
      0.5001  0.6686  0.8542  0.2808  0.5107
      0.7807  0.9526  0.7597  0.6664  0.3614
      0.4027  0.0670  0.1209  0.0648  0.8693
      0.1669  0.4058  0.9315  0.4091  0.8107
      0.4265  0.5755  0.8383  0.3496  0.7982
     [torch.DoubleTensor of size 5x5]

                                                                           [0.0001s]	
                                                                           [0.0000s]	
                                                                           [0.0000s]	
      0.4027  0.0670  0.1209  0.0648  0.8693
      0.5001  0.6686  0.8542  0.2808  0.5107
     [torch.DoubleTensor of size 2x5]

                                                                           [0.0001s]	
                                                                           [0.0000s]	
      1  1  1  1  1
      1  1  1  1  1
     [torch.DoubleTensor of size 2x5]

                                                                           [0.0001s]	
                                                                           [0.0000s]	
      1  1  1  1  1
      1  1  1  1  1
     [torch.DoubleTensor of size 2x5]

                                                                           [0.0001s]	
      0.5001  0.6686  0.8542  0.2808  0.5107
      0.7807  0.9526  0.7597  0.6664  0.3614
      0.4027  0.0670  0.1209  0.0648  0.8693
      0.1669  0.4058  0.9315  0.4091  0.8107
      0.4265  0.5755  0.8383  0.3496  0.7982
     [torch.DoubleTensor of size 5x5]

                                                                           [0.0001s]	
     Do you really want to exit ([y]/n)? 	
#+end_example

*** neural style
**** TODO [[https://github.com/jcjohnson/neural-style.git][neural style in torch]]
     SCHEDULED: <2016-10-11 Tue>
**** TODO [[https://github.com/jcjohnson/fast-neural-style][fast-neural-style in torch]]
** Lua
*** =__index= metamethod
    当你通过键来访问 =table= 的时候，如果这个键没有值，那么 =Lua= 就会
    寻找该 =table= 的 =metatable= （假定有 =metatable= ）中的 =__index= 键。
    如果 =__index= 包含一个表格， =Lua= 会在表格中查找相应的键。 
    #+BEGIN_SRC lua :results output
      other = { foo = 3 }
      t = setmetatable({}, { __index = other })
      print(t.foo)

      print(t.bar)
    #+END_SRC

    #+RESULTS:
    : 3
    : nil

    如果 =__index= 包含一个函数的话， =Lua= 就会调用那个函数， =table=
    和键会作为参数传递给函数。 =__index= 元方法查看表中元素是否存在，如果
    不存在，返回结果为 =nil= ；如果存在则由 =__index= 返回结果。
    #+BEGIN_SRC lua :results output
      mytable = setmetatable({key1 = "value1"}, {
        __index = function(mytable, key)
          if key == "key2" then
            return "metatablevalue"
          else
            return nil
          end
        end
      })

      print(mytable.key1,mytable.key2)
    #+END_SRC

    #+RESULTS:
    : value1	metatablevalue

    1. 在表中查找，如果找到，返回该元素，找不到则继续
    2. 判断该表是否有元表，如果没有元表，返回 =nil= ，有元表则继续。
    3. 判断元表有没有 =__index= 方法，如果 =__index= 方法为 =nil= ，则返回 =nil= ；如
       果 =__index= 方法是一个表，则重复1、2、3；如果 =__index= 方法是一个函数，
       则返回该函数的返回值。
*** =__newindex= 
    =__newindex= 元方法用来对表更新 =__newindex= 则用来对表访问 。
    当你给表的一个缺少的索引赋值，解释器就会查找 =__newindex= 元方法：如
    果存在则调用这个函数而不进行赋值操作。 
    #+BEGIN_SRC lua :results output
      mymetatable = {}
      mytable = setmetatable({key1 = "value1"}, { __newindex = mymetatable })

      print(mytable.key1)

      mytable.newkey = "新值2"
      print(mytable.newkey,mymetatable.newkey)

      mytable.key1 = "新值1"
      print(mytable.key1,mymetatable.key1)
    #+END_SRC

    #+RESULTS:
    : value1
    : nil	新值2
    : 新值1	nil


    以上实例中表设置了元方法 =__newindex= ，在对新索引键（newkey）赋值时
    （mytable.newkey = "新值2"），会调用元方法，而不进行赋值。而如果对
    已存在的索引键（key1），则会进行赋值，而不调用元方法 =__newindex=
    。 
    #+RESULTS:
    : new value	"4"

    #+BEGIN_SRC lua :results output
      mytable = setmetatable({key1 = "value1"}, {
        __newindex = function(mytable, key, value)
              rawset(mytable, key, "\""..value.."\"")

        end
      })

      mytable.key1 = "new value"
      mytable.key2 = 4

      print(mytable.key1,mytable.key2)
    #+END_SRC
*** iterate =string=
    #+BEGIN_SRC lua :results output
      a = 'fds.fd.ds'

      for char in a:gmatch"." do
         print(char)
      end
    #+END_SRC

    #+RESULTS:
    : f
    : d
    : s
    : .
    : f
    : d
    : .
    : d
    : s
*** [[http://www.newthinktank.com/2015/06/learn-lua-one-video/][one video tutorial for lua]]
** Git
*** [[https://www.quora.com/What-is-the-best-Git-cheat-sheet][git cheatsheet]]
*** Git stah
**** TODO [[https://git-scm.com/book/zh/v1/Git-%25E5%25B7%25A5%25E5%2585%25B7-%25E5%2582%25A8%25E8%2597%258F%25EF%25BC%2588Stashing%25EF%25BC%2589][tutorial]]
*** ssh-key password
    * use ssh address not https
    #+BEGIN_SRC sh
      git remote set-url origin git@github.com:yourname/yourrepo.git
    #+END_SRC
*** Creating mutilple github accounts
    1. create another ssh key for a completely new account on Github
       #+BEGIN_SRC sh
         eval `ssh-agent -s`
         ssh-keygen -t rsa -C "Your Email Address" # Generates the key
       #+END_SRC
    2. Give it a name (e.g. xinyixiao)
    3. A public key and randomart are generted again
    4. copy public key to github for setting
    5. We used a unique name for our keys so we have to tell ssh about them.
       #+BEGIN_SRC sh
         ssh-add ~/.ssh/xixaoxinyi
         ssh-add ~/.ssh/xinyixiao
       #+END_SRC
    6. touch ~/.ssh/config
    7. define which account we want to work with by associating a keyword to our 2 different hosts.
       Host github.com
       HostName github.com
       User xiaoxinyi
       IdentifyFile ~/.ssh/xiaoxinyi

       Host github.com
       HostName github-aliyun
       User xinyixiao
       IdentifyFile ~/.ssh/xinyixiao
    8. git remote add origin git@github-aliyun:xinyixiao/git-aliyun.git
    9. git push origin master
*** Fork and Pull workflow
    * The way the Fork & Pull works is that anyone can Fork a
      repository and make changes locally. They don't have the
      ability to push their potentially damaging code. They can
      however request that host repository pull their changes if they
      would like using a Pull Request. (This is a very common
      workflow in the open source community.)
      1. git remote add upstream
         git@github.com/xiaoxinyi/git-aliyun.git
         # Assign the original remote and not the fork to the keyword
         # upstream
      2. git fetch upstream
         # Pull in change a file locally, stage and commit it. I can
         # the push it to more Fork on github
      3. git merge upstream/master  # merges files on github with my
         local files
      4. If I think my changes should be merged with the original
         repository I can make a Pull Request. Click on Compare,
         review, create a pull request on GitHub.
      5. The owner of the original repository can see how many Pull
         Requests they have received on the right side of the
         screen. 
*** [[https://www.atlassian.com/git/tutorials/syncing][git sync tutorial]]
*** ssh-key password
    * use ssh address not https
    #+BEGIN_SRC sh
      git remote set-url origin git@github.com:yourname/yourrepo.git
    #+END_SRC
*** Creating mutilple github accounts
    1. create another ssh key for a completely new account on Github
       #+BEGIN_SRC sh
         ssh-keygen -t rsa -C "Your Email Address" # Generates the key
       #+END_SRC
    2. Give it a name (e.g. xinyixiao)
    3. A public key and randomart are generted again
    4. copy public key to github for setting
    5. We used a unique name for our keys so we have to tell ssh about them.
       #+BEGIN_SRC sh
         ssh-add ~/.ssh/xixaoxinyi
         ssh-add ~/.ssh/xinyixiao
       #+END_SRC
    6. touch ~/.ssh/config
    7. define which account we want to work with by associating a keyword to our 2 different hosts.
       Host github.com
       HostName github.com
       User xiaoxinyi
       IdentifyFile ~/.ssh/xiaoxinyi

       Host github.com
       HostName github-aliyun
       User xinyixiao
       IdentifyFile ~/.ssh/xinyixiao
    8. git remote add origin git@github-aliyun:xinyixiao/git-aliyun.git
    9. git push origin master
*** Fork and Pull workflow
    * The way the Fork & Pull works is that anyone can Fork a
      repository and make changes locally. They don't have the
      ability to push their potentially damaging code. They can
      however request that host repository pull their changes if they
      would like using a Pull Request. (This is a very common
      workflow in the open source community.)
      1. git remote add upstream
         git@github.com/xiaoxinyi/git-aliyun.git
         # Assign the original remote and not the fork to the keyword
         # upstream
      2. git fetch upstream
         # Pull in change a file locally, stage and commit it. I can
         # the push it to more Fork on github
      3. git merge upstream/master  # merges files on github with my
         local files
      4. If I think my changes should be merged with the original
         repository I can make a Pull Request. Click on Compare,
         review, create a pull request on GitHub.
      5. The owner of the original repository can see how many Pull
         Requests they have received on the right side of the
         screen. 
*** git branch cheatsheet
    #+BEGIN_SRC sh
      # create a new branch
      git branch feature
      # switch to feature branch
      git chechout feature
      # push branch feature to remote
      git push origin feature
      # show all braches both locally and remotely
      git branch -a
      # delete merged branch
      git branch -d feature
      # delete unmerged branch
      git branch -D feature
      # delete remote branch
      git push --delete origin feature
    #+END_SRC
*** git add
    #+BEGIN_SRC sh
      # add only  modified files
      git ls-files --modified | xargs git add
    #+END_SRC
*** git checkout
    #+BEGIN_SRC sh
      # pull a remote branch and create the same local branch
      git checkout --track origin/raychar
    #+END_SRC
*** git cherry-pick
    #+BEGIN_SRC sh
      # Copy branch to another branch
      git cherry-pick hash
    #+END_SRC
*** git diff
    #+BEGIN_SRC sh
      # diff unstaged files with last commit
      git diff

      # diff staged files with last commit
      git diff --cached
    #+END_SRC
*** git log
    #+BEGIN_SRC sh
      # most used log command
      git log --oneline --decorate --all --graph
    #+END_SRC
*** git merge
    #+BEGIN_SRC sh
      # Discard current merge
      git merge --abort

      # merge branch
      git merge branch

      # merge non fast forward
      git metge --no --FF
    #+END_SRC
*** git ref
**** push branch different names
     Refspecs can be used with the git push command to give a different
     name to the remote branch. For example, the following command
     pushes the master branch to the origin remote repo like an
     ordinary git push, but it uses qa-master as the name for the
     branch in the origin repo  
     #+BEGIN_SRC sh
       git push origin master:refs/heads/qa-master
     #+END_SRC
**** non-fast-forward update remote repository
     A refspec is specified as [+]<src>:<dst>. The <src> parameter is
     the source branch in the local repository, and the <dst> parameter
     is the destination branch in the remote repository. The optional +
     sign is for forcing the remote repository to perform a
     non-fast-forward update. 
**** delete remote branches 
     #+BEGIN_SRC sh
       # use refspecs for deleting remote branches
       git push origin :some-feature
       git push origin --delete some-feature
     #+END_SRC
**** fetch specific branched
     By default, git fetch fetches all of the branches in the remote repository. The reason for this is the following section of the .git/config file
     [remote "origin"]
       url = https://git@github.com:mary/example-repo.git
       fetch = +refs/heads/*:refs/remotes/origin/*

     To fetch only the master branch, change the fetch line to match the following
        [remote "origin"]
            url = https://git@github.com:mary/example-repo.git
            fetch = +refs/heads/master:refs/remotes/origin/master

     if you want to always push the master branch to qa-master in the origin remote
        [remote "origin"]
            url = https://git@github.com:mary/example-repo.git
            fetch = +refs/heads/master:refs/remotes/origin/master
            push = refs/heads/master:refs/heads/qa-master
**** git head
     #+BEGIN_SRC sh
       # grandparent of HEAD
       git show HEAD~2

       # the first parent is from the branch that you were on when you performed the merge, and the second parent is from the branch that you passed to the git merge command.
       # first parent merge second parent branch

       # The ~ character will always follow the first parent of a merge commit. If you want to follow a different parent, you need to specify which one with the ^ character.

       # if HEAD is a merge commit, the following returns the second parent of HEAD
       git show HEAD^2


       # Only list commits that are parent of the second parent of a merge commit
       git log HEAD^2

       # Remove the last 3 commits from the current branch
       git reset HEAD~3

       # Interactively rebase the last 3 commits on the current branch
       git rebase -i HEAD~3
     #+END_SRC
**** git rest
     #+BEGIN_SRC sh
       git reset --hard HEAD  # current commit
       git reset --hard HEAD^  # parent commit
       git reset --hard HEAD^^ # grandparet commit

       # Remove the specified file from the staging area, but leave the working directory unchanged.
       git reset <file>
       # Reset the staging area to match the most recent commit, but leave the working directory unchanged.
       git reset
       # Reset the staging area and the working directory to match the most recent commit.
       git reset --hard
     #+END_SRC
**** git rm
     #+BEGIN_SRC sh
       # rm files from working space
       git rm <file>

       # untracked files but still in working space
       git rm --cached <file>
     #+END_SRC
**** git rev-parse
     #+BEGIN_SRC sh
       # The following returns the hash of the commit pointed to by the master branch
       git rev-parse master
     #+END_SRC
**** git show
     #+BEGIN_SRC sh
       # bring back deleted file from some version
       git show commit_id:path/to/file > path/to/file
     #+END_SRC
**** git stash
     #+BEGIN_SRC sh
       ## By default, running git stash will stash:
       # changes that have been added to your index (staged changes)
       # changes made to files that are currently tracked by Git (unstaged changes)


       ## But it will not stash:
       # new files in your working copy that have not yet been staged
       # files that have been ignored

       git stash # stash staged file and tracked but unstaged file
       git stash -u # stash untrack file
       git stash -a # stash ignore file


       # Creating a branch from your stash
        git stash branch add-style stash@{1}
       # Cleaning up your stash
       git stash drop stash@{1}
       git stash clear

       git stash list
       git stash save "add style to our site"

       # Viewing stash diffs
       git stash show
       git stash show -p

       # Partial stashes
       git stash -p

       # rename existing stash

       git stash drop stash@{1}
       git stash store -m 'refactor virturalenvironment.h|m to virtualenvironment.hh|mm' 0beef13ff5a36dccfee1c58cf7ad1508df524771

       # stash untracked files with a stash msg
       git stash save -u 'untracked files examples/ARAppCore/VirtualEnvironment.hh|mm'
     #+END_SRC
**** git submodule
     #+BEGIN_SRC sh
       git submodule add ../xx.git submod
       git submodule update
     #+END_SRC
**** git update-index
     #+BEGIN_SRC sh
       # This will tell git you want to start ignoring the changes to the file
       git update-index --assume-unchanged path/to/file

       # When you want to start keeping track again
       git update-index --no-assume-unchanged path/to/file
     #+END_SRC
**** gitignore
     #+BEGIN_SRC sh
       # Ignoring a previously committed file
       echo debug.log >> .gitignore
       git rm --cached debug.log
       git commit -m "Start ignoring debug.log"

       # Global Git ignore rules
       touch ~/.gitignore
       git config --global core.excludesFile ~/.gitignore

       # Committing an ignored file
       git add -f debug.log
       git commit -m "Force adding debug.log"

       echo !debug.log >> .gitignore
       git add debug.log
       git commit -m "Adding debug.log"

       # Debugging .gitignore files
       git check-ignore -v debug.log
     #+END_SRC
*** git clone
    #+BEGIN_SRC sh
      # clone a specific branch
      git clone -b <branch> <remote_repo>
    #+END_SRC
*** Questions
**** pull wrong branch into master
     #+BEGIN_SRC sh
       git reflog

       git reset --hard <hash>
     #+END_SRC
** C++
*** TODO [[/Users/zhangli/Documents/Library.papers3/Files/2A/2A75498D-39F3-4E24-A6F0-5CE79A0A5A11.pdf][C++ concurrency in action]]
    SCHEDULED: <2016-09-06 二>
**** [[https://www.gitbook.com/book/chenxiaowei/cpp_concurrency_in_action/details][resource for book]]
** Alfred
*** [[http://www.alfredworkflow.com/][workflow repository]]
** Latex
*** [[http://latex.wikia.com/wiki/List_of_LaTeX_symbols][reference for symbol]]
** VPN
*** tutorial
**** [[https://www.digitalocean.com/community/tutorials/how-to-set-up-an-openvpn-server-on-ubuntu-16-04][openvpn on ubuntu 16.04]]
** Octave
*** install octave
    #+BEGIN_SRC sh
      brew install gnuplot --with-aquaterm --with-qt5
    #+END_SRC
    add setenv('GNUTERM','aqua') /usr/local/Cellar/octave/4.2.0-rc2_1/share/octave/site/m/startup
** Objective-C
*** delegate
**** MyClass.h file should look like this (add delegate lines with comments!)
     #+BEGIN_SRC c
       #import <BlaClass/BlaClass.h>

       @class MyClass;             //define class, so protocol can see MyClass
       @protocol MyClassDelegate <NSObject>   //define delegate protocol
           - (void) myClassDelegateMethod: (MyClass *) sender;  //define delegate method to be implemented within another class
       @end //end protocol

       @interface MyClass : NSObject {
       }
       @property (nonatomic, weak) id <MyClassDelegate> delegate; //define MyClassDelegate as delegate

       @end
     #+END_SRC
**** MyClass.m file should look like this
       #+BEGIN_SRC c
         #import "MyClass.h"
         @implementation MyClass
         @synthesize delegate; //synthesise  MyClassDelegate delegate

         - (void) myMethodToDoStuff {
             [self.delegate myClassDelegateMethod:self]; //this will call the method implemented in your other class
         }

         @end
       #+END_SRC
**** To use your delegate in another class (UIViewController called MyVC in this case) MyVC.h:
        #+BEGIN_SRC c
          #import "MyClass.h"
          @interface MyVC:UIViewController <MyClassDelegate> { //make it a delegate for MyClassDelegate
          }
        #+END_SRC
**** MyVC.m:
        #+BEGIN_SRC c
          myClass.delegate = self;          //set its delegate to self somewhere

          // Implement delegate method

          - (void) myClassDelegateMethod: (MyClass *) sender {
              NSLog(@"Delegates are great!");
          }
        #+END_SRC
If I were to explain delegates to a C++/Java programmer I would say 
What are delegates ? These are static pointers to classes within
another class. Once you assign a pointer, you can call
functions/methods in that class. Hence some functions of your class
are "delegated" (In C++ world - pointer to by a class object pointer)
to another class. 
